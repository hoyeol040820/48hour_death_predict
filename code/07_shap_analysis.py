#!/usr/bin/env python3
"""
SHAP Analysis
- TreeSHAPìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤ì˜ feature importance ë¶„ì„
- Global/Local í•´ì„ì„± ì œê³µ
- ìƒìœ„ ëª¨ë¸ë“¤ì˜ feature interaction ë¶„ì„
"""

import pandas as pd
import numpy as np
import joblib
from pathlib import Path
import warnings
import json
warnings.filterwarnings('ignore')

# SHAP íŒ¨í‚¤ì§€ í™•ì¸
try:
    import shap
    # SHAP ì„¤ì •
    shap.initjs()
    SHAP_AVAILABLE = True
except ImportError:
    print("âš ï¸ SHAP íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install shap")
    SHAP_AVAILABLE = False

class SHAPAnalyzer:
    def __init__(self, modeling_results_path, data_path, output_path, tuning_results_path=None):
        self.modeling_results_path = Path(modeling_results_path)
        self.data_path = Path(data_path)
        self.output_path = Path(output_path)
        self.tuning_results_path = Path(tuning_results_path) if tuning_results_path else None
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # SHAP ê²°ê³¼ ì €ì¥ìš©
        self.shap_results = {}
        
    def load_modeling_results(self):
        """ëª¨ë¸ë§ ê²°ê³¼ ë¡œë“œ"""
        print("ğŸ“Š ëª¨ë¸ë§ ê²°ê³¼ ë¡œë“œ ì¤‘...")
        
        results_df = pd.read_csv(self.modeling_results_path)
        print(f"âœ… ì´ {len(results_df)}ê°œ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ")
        
        return results_df
    
    def load_tuning_results(self):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ë¡œë“œ"""
        if self.tuning_results_path and self.tuning_results_path.exists():
            print("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ë¡œë“œ ì¤‘...")
            tuning_df = pd.read_csv(self.tuning_results_path)
            print(f"âœ… ì´ {len(tuning_df)}ê°œ íŠœë‹ëœ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ")
            return tuning_df
        else:
            print("âš ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    def select_top_models(self, results_df, top_k=5):
        """íŠ¹ì • ëª¨ë¸ë“¤ ì„ íƒ (XGBoost, LightGBMì˜ original, SMOTEë§Œ)"""
        print("ğŸ† XGBoost, LightGBMì˜ original, SMOTE ëª¨ë¸ë“¤ ì„ íƒ...")
        
        # XGBoost, LightGBMë§Œ í•„í„°ë§
        target_models = ['XGBoost', 'LightGBM']
        target_resampling = ['original', 'smote']
        
        # ì¡°ê±´ì— ë§ëŠ” ëª¨ë¸ë“¤ í•„í„°ë§
        filtered_results = results_df[
            (results_df['model_name'].isin(target_models)) & 
            (results_df['resampling_method'].isin(target_resampling))
        ].copy()
        
        # ROC-AUC ê¸°ì¤€ ì •ë ¬
        top_models = filtered_results.sort_values('roc_auc', ascending=False)
        
        print("ì„ íƒëœ ëª¨ë¸ë“¤:")
        for idx, row in top_models.iterrows():
            print(f"  {row['resampling_method'].upper()}-{row['model_name']}: ROC-AUC {row['roc_auc']:.4f}")
        
        return top_models
    
    def select_tuned_models(self, tuning_df):
        """íŠœë‹ëœ ëª¨ë¸ë“¤ ì„ íƒ (XGBoost, LightGBMì˜ original, SMOTEë§Œ)"""
        if tuning_df is None:
            return None
            
        print("ğŸ¯ íŠœë‹ëœ XGBoost, LightGBMì˜ original, SMOTE ëª¨ë¸ë“¤ ì„ íƒ...")
        
        # base_modelê³¼ resampling_method ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
        target_models = ['XGBoost', 'LightGBM']
        target_resampling = ['original', 'smote']
        
        filtered_results = tuning_df[
            (tuning_df['base_model'].isin(target_models)) & 
            (tuning_df['resampling_method'].isin(target_resampling))
        ].copy()
        
        # ROC-AUC ê¸°ì¤€ ì •ë ¬
        tuned_models = filtered_results.sort_values('roc_auc', ascending=False)
        
        print("ì„ íƒëœ íŠœë‹ëœ ëª¨ë¸ë“¤:")
        for idx, row in tuned_models.iterrows():
            print(f"  {row['resampling_method'].upper()}-{row['base_model']}: ROC-AUC {row['roc_auc']:.4f} (íŠœë‹ë¨)")
        
        return tuned_models
    
    def load_model_and_data(self, model_info):
        """ëª¨ë¸ê³¼ í•´ë‹¹ ë°ì´í„° ë¡œë“œ"""
        model_path = model_info['model_path']
        resampling_method = model_info['resampling_method']
        # íŠœë‹ëœ ëª¨ë¸ì¸ ê²½ìš° base_model ì‚¬ìš©, ì•„ë‹ˆë©´ model_name ì‚¬ìš©
        model_name = model_info.get('base_model', model_info.get('model_name', 'Unknown'))
        
        print(f"ğŸ”„ {resampling_method.upper()}-{model_name} ë¡œë”© ì¤‘...")
        
        # ëª¨ë¸ ë¡œë“œ
        try:
            model = joblib.load(model_path)
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None, None, None
        
        # ë°ì´í„° ë¡œë“œ
        data_base_path = self.data_path / resampling_method
        
        try:
            train_df = pd.read_csv(data_base_path / "mimic_mortality_train.csv")
            test_df = pd.read_csv(data_base_path / "mimic_mortality_test.csv")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None, None, None
        
        # ì „ì²˜ë¦¬ (05_modeling_evaluation.pyì™€ ë™ì¼í•œ ë°©ì‹)
        target_column = 'mortality_48h'
        id_columns = ['subject_id', 'hadm_id', 'stay_id']
        existing_id_cols = [col for col in id_columns if col in train_df.columns]
        
        X_train = train_df.drop(columns=existing_id_cols + [target_column])
        X_test = test_df.drop(columns=existing_id_cols + [target_column])
        y_test = test_df[target_column]
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ ì¸ì½”ë”© (Trainìœ¼ë¡œë§Œ fit)
        from sklearn.preprocessing import LabelEncoder
        
        categorical_columns = X_train.select_dtypes(include=['object']).columns
        if len(categorical_columns) > 0:
            for col in categorical_columns:
                le = LabelEncoder()
                le.fit(X_train[col].astype(str))
                
                # Safe transform function
                def safe_transform(series, encoder):
                    result = series.astype(str).copy()
                    mask = result.isin(encoder.classes_)
                    result_encoded = np.full(len(result), -1, dtype=int)
                    result_encoded[mask] = encoder.transform(result[mask])
                    return result_encoded
                
                X_train[col] = le.transform(X_train[col].astype(str))
                X_test[col] = safe_transform(X_test[col], le)
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        if X_train.isnull().any().any():
            for col in X_train.columns:
                if X_train[col].isnull().any():
                    mean_val = X_train[col].mean()
                    X_train[col].fillna(mean_val, inplace=True)
                    X_test[col].fillna(mean_val, inplace=True)
        
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: Train {X_train.shape}, Test {X_test.shape}")
        return model, X_train, X_test, y_test
    
    def calculate_shap_values(self, model, X_train, X_test, model_info, sample_size=1000):
        """SHAP ê°’ ê³„ì‚°"""
        base_model_name = model_info.get('base_model', model_info.get('model_name', 'Unknown'))
        model_name = f"{model_info['resampling_method']}_{base_model_name}"
        print(f"ğŸ” {model_name} SHAP ë¶„ì„ ì¤‘...")
        
        # ìƒ˜í”Œ í¬ê¸° ì¡°ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
        if len(X_train) > sample_size:
            background_sample = X_train.sample(n=sample_size, random_state=42)
        else:
            background_sample = X_train.copy()
        
        if len(X_test) > sample_size:
            test_sample = X_test.sample(n=sample_size, random_state=42)
        else:
            test_sample = X_test.copy()
        
        try:
            # TreeExplainer ì‚¬ìš©
            explainer = shap.TreeExplainer(model)
            
            # SHAP values ê³„ì‚°
            shap_values = explainer.shap_values(test_sample)
            
            print(f"   Raw SHAP values type: {type(shap_values)}")
            if isinstance(shap_values, list):
                print(f"   SHAP values list length: {len(shap_values)}")
                for i, sv in enumerate(shap_values):
                    print(f"   SHAP values[{i}] shape: {np.array(sv).shape}")
            else:
                print(f"   SHAP values shape: {np.array(shap_values).shape}")
            
            # XGBoost/LightGBM/RandomForestì˜ ê²½ìš° ì²˜ë¦¬
            if isinstance(shap_values, list):
                if len(shap_values) == 2:
                    # ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš° positive class (index 1) ì‚¬ìš©
                    shap_values = shap_values[1]
                else:
                    # ë‹¨ì¼ í´ë˜ìŠ¤ì¸ ê²½ìš° ì²« ë²ˆì§¸ ì‚¬ìš©
                    shap_values = shap_values[0]
            
            # numpy arrayë¡œ ë³€í™˜
            shap_values = np.array(shap_values)
            
            # ì°¨ì› í™•ì¸
            if shap_values.ndim != 2:
                print(f"   âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ SHAP values ì°¨ì›: {shap_values.shape}")
                # 2ì°¨ì›ìœ¼ë¡œ ë§Œë“¤ê¸°
                if shap_values.ndim == 1:
                    shap_values = shap_values.reshape(1, -1)
                elif shap_values.ndim > 2:
                    # ì²« ë‘ ì°¨ì›ë§Œ ì‚¬ìš©
                    shap_values = shap_values.reshape(shap_values.shape[0], -1)
            
            # ê¸°ëŒ“ê°’
            expected_value = explainer.expected_value
            if isinstance(expected_value, list):
                expected_value = expected_value[1] if len(expected_value) > 1 else expected_value[0]
            
            print(f"âœ… SHAP ê°’ ê³„ì‚° ì™„ë£Œ: {shap_values.shape}")
            
            # ê²°ê³¼ ì €ì¥
            shap_result = {
                'model_name': model_name,
                'shap_values': shap_values,
                'test_sample': test_sample,
                'expected_value': expected_value,
                'feature_names': list(X_test.columns),
                'model_info': model_info
            }
            
            self.shap_results[model_name] = shap_result
            return shap_result
            
        except Exception as e:
            print(f"âŒ SHAP ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_feature_importance(self, shap_result):
        """Feature importance ë¶„ì„"""
        model_name = shap_result['model_name']
        shap_values = shap_result['shap_values']
        feature_names = shap_result['feature_names']
        
        print(f"ğŸ“Š {model_name} Feature Importance ë¶„ì„...")
        print(f"   SHAP values shape: {shap_values.shape}")
        
        # Global feature importance (í‰ê·  ì ˆëŒ“ê°’)
        feature_importance = np.mean(np.abs(shap_values), axis=0)
        
        # ì°¨ì› í™•ì¸ ë° í‰íƒ„í™”
        if feature_importance.ndim > 1:
            feature_importance = feature_importance.flatten()
        
        print(f"   Feature importance shape: {feature_importance.shape}")
        print(f"   Feature names count: {len(feature_names)}")
        
        # ê¸¸ì´ ì¼ì¹˜ í™•ì¸
        if len(feature_importance) != len(feature_names):
            min_len = min(len(feature_importance), len(feature_names))
            feature_importance = feature_importance[:min_len]
            feature_names = feature_names[:min_len]
            print(f"   âš ï¸ ê¸¸ì´ ë¶ˆì¼ì¹˜ - {min_len}ê°œë¡œ ì¡°ì •")
        
        # DataFrameìœ¼ë¡œ ì •ë¦¬
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        # ìƒìœ„ 20ê°œ íŠ¹ì„±
        top_features = importance_df.head(20)
        
        print(f"ìƒìœ„ 10ê°œ íŠ¹ì„±:")
        for idx, row in top_features.head(10).iterrows():
            print(f"  {row['feature']}: {row['importance']:.4f}")
        
        return importance_df, top_features
    
    def save_shap_results(self):
        """SHAP ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        print("\nğŸ’¾ SHAP ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ê° ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥
        for model_name, shap_result in self.shap_results.items():
            # Feature importance ë¶„ì„
            importance_df, top_features = self.analyze_feature_importance(shap_result)
            
            # CSV ì €ì¥
            importance_path = self.output_path / f"{model_name}_feature_importance.csv"
            importance_df.to_csv(importance_path, index=False)
            
            # SHAP ê°’ ì €ì¥ (ìƒìœ„ íŠ¹ì„±ë§Œ)
            top_feature_names = top_features['feature'].tolist()
            shap_values = shap_result['shap_values']
            test_sample = shap_result['test_sample']
            
            # ìƒìœ„ íŠ¹ì„±ë“¤ì˜ SHAP ê°’ë§Œ ì €ì¥
            top_indices = [shap_result['feature_names'].index(f) for f in top_feature_names if f in shap_result['feature_names']]
            top_shap_values = shap_values[:, top_indices]
            
            # ì €ì¥ìš© ë°ì´í„°
            shap_data = {
                'model_name': model_name,
                'feature_names': top_feature_names,
                'shap_values': top_shap_values.tolist(),
                'expected_value': float(shap_result['expected_value']),
                'model_info': {
                    'resampling_method': shap_result['model_info']['resampling_method'],
                    'model_name': shap_result['model_info']['model_name'],
                    'roc_auc': float(shap_result['model_info']['roc_auc'])
                }
            }
            
            # JSONìœ¼ë¡œ ì €ì¥
            shap_path = self.output_path / f"{model_name}_shap_values.json"
            with open(shap_path, 'w') as f:
                json.dump(shap_data, f, indent=2)
            
            print(f"âœ… {model_name} ê²°ê³¼ ì €ì¥: {importance_path}")
        
        # ì „ì²´ ìš”ì•½ ìƒì„±
        self.create_summary_report()
    
    def create_summary_report(self):
        """SHAP ë¶„ì„ ìš”ì•½ ë³´ê³ ì„œ"""
        summary_path = self.output_path / "shap_analysis_summary.txt"
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("MIMIC-IV 48ì‹œê°„ ì‚¬ë§ë¥  ì˜ˆì¸¡ - SHAP ë¶„ì„ ê²°ê³¼\n")
            f.write("=" * 60 + "\n")
            f.write(f"ë¶„ì„ ì¼ì‹œ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("ë¶„ì„ ê°œìš”:\n")
            f.write("- ë°©ë²•: TreeSHAP (Tree-based ëª¨ë¸ ì „ìš©)\n") 
            f.write("- ëŒ€ìƒ: ìƒìœ„ ì„±ëŠ¥ Tree-based ëª¨ë¸ë“¤\n")
            f.write("- í•´ì„ì„±: Global feature importance + Local explanations\n\n")
            
            f.write(f"ë¶„ì„ ëª¨ë¸ ({len(self.shap_results)}ê°œ):\n")
            for model_name, shap_result in self.shap_results.items():
                model_info = shap_result['model_info']
                f.write(f"- {model_name}: ROC-AUC {model_info['roc_auc']:.4f}\n")
            
            f.write("\nìƒìœ„ íŠ¹ì„± ìš”ì•½ (ëª¨ë¸ë³„ ìƒìœ„ 5ê°œ):\n")
            f.write("-" * 40 + "\n")
            
            for model_name, shap_result in self.shap_results.items():
                importance_df, _ = self.analyze_feature_importance(shap_result)
                top_5 = importance_df.head(5)
                
                f.write(f"\n{model_name}:\n")
                for idx, row in top_5.iterrows():
                    f.write(f"  {idx+1}. {row['feature']}: {row['importance']:.4f}\n")
            
            f.write(f"\nì €ì¥ íŒŒì¼:\n")
            f.write(f"- Feature importance: *_feature_importance.csv\n")
            f.write(f"- SHAP values: *_shap_values.json\n")
            f.write(f"- ì‹œê°í™”: figure_generator.py ì‹¤í–‰ ì‹œ ìƒì„±\n")
        
        print(f"âœ… ìš”ì•½ ë³´ê³ ì„œ: {summary_path}")

    def run_analysis(self, top_k=5):
        """ì „ì²´ SHAP ë¶„ì„ ì‹¤í–‰ (íŠœë‹ëœ ëª¨ë¸ë“¤ë§Œ)"""
        print("ğŸš€ SHAP ë¶„ì„ ì‹œì‘ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ëœ ëª¨ë¸ë“¤)")
        print("=" * 60)
        
        # SHAP íŒ¨í‚¤ì§€ ê°€ìš©ì„± í™•ì¸
        if not SHAP_AVAILABLE:
            print("âŒ SHAP íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        # 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ë¡œë“œ
        tuning_df = self.load_tuning_results()
        if tuning_df is None:
            print("âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. íŠœë‹ëœ ëª¨ë¸ë“¤ ì„ íƒ (XGBoost, LightGBMì˜ original, SMOTEë§Œ)
        selected_models = self.select_tuned_models(tuning_df)
        if selected_models is None or len(selected_models) == 0:
            print("âŒ ë¶„ì„í•  íŠœë‹ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 3. ê° ëª¨ë¸ë³„ SHAP ë¶„ì„
        for idx, (_, model_info) in enumerate(selected_models.iterrows()):
            model_display_name = f"{model_info['resampling_method']}-{model_info['base_model']}"
            print(f"\nğŸ“Š ëª¨ë¸ {idx+1}/{len(selected_models)}: {model_display_name} (íŠœë‹ë¨)")
            
            try:
                # ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ
                model, X_train, X_test, y_test = self.load_model_and_data(model_info)
                
                if model is None:
                    print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                    continue
                
                # SHAP ë¶„ì„
                shap_result = self.calculate_shap_values(model, X_train, X_test, model_info)
                
                if shap_result is None:
                    print(f"âš ï¸ SHAP ë¶„ì„ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                    continue
                    
            except Exception as e:
                print(f"âŒ ëª¨ë¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                model_name = model_info.get('base_model', model_info.get('model_name', 'Unknown'))
                print(f"âš ï¸ {model_info['resampling_method']}-{model_name} ê±´ë„ˆëœ€")
                continue
        
        # 4. ê²°ê³¼ ì €ì¥
        if self.shap_results:
            self.save_shap_results()
            print(f"\nâœ… SHAP ë¶„ì„ ì™„ë£Œ! ì´ {len(self.shap_results)}ê°œ ëª¨ë¸ ë¶„ì„")
        else:
            print("\nâŒ ë¶„ì„ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” SHAP ë¶„ì„ ì‹œì‘")
    print("=" * 60)
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
    project_root = Path(__file__).parent.parent
    
    # ê²½ë¡œ ì„¤ì • - ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
    modeling_results_path = project_root / "dataset" / "4_modeling" / "modeling_results.csv"
    tuning_results_path = project_root / "dataset" / "5_final_models" / "tuning_results.csv"
    data_path = project_root / "dataset" / "3_resampled"
    output_path = project_root / "results" / "07_shap_analysis"
    
    # SHAP ë¶„ì„ê¸° ì´ˆê¸°í™” (íŠœë‹ ê²°ê³¼ ê²½ë¡œ í¬í•¨)
    analyzer = SHAPAnalyzer(modeling_results_path, data_path, output_path, tuning_results_path)
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer.run_analysis(top_k=5)
    
    print("\n" + "=" * 60)
    print("âœ… SHAP ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_path}")
    print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: figure_generator.py ì‹¤í–‰í•˜ì—¬ ì‹œê°í™” ìƒì„±")
    print("=" * 60)

if __name__ == "__main__":
    main()
