#!/usr/bin/env python3
"""
Figure Generator
ì „ì²´ ë¶„ì„ ê³¼ì •ì˜ ëª¨ë“  ì‹œê°í™”ë¥¼ í•œ ë²ˆì— ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ìƒì„± ëŒ€ìƒ:
1. ë°ì´í„° ë¶„í¬ ì‹œê°í™” 
2. ê²°ì¸¡ì¹˜ ë¶„ì„ íˆíŠ¸ë§µ
3. í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„êµ
4. ë¦¬ìƒ˜í”Œë§ íš¨ê³¼ ë¹„êµ
5. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
6. ìµœì¢… ê²°ê³¼ ëŒ€ì‹œë³´ë“œ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import json
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

class FigureGenerator:
    def __init__(self):
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        self.base_path = Path(__file__).parent.parent
        self.dataset_path = self.base_path / "dataset"
        self.output_path = self.base_path / "figures"
        self.output_path.mkdir(exist_ok=True)
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì„¤ì •
        self.colors = {
            'primary': '#2E86C1',
            'secondary': '#F39C12', 
            'success': '#27AE60',
            'danger': '#E74C3C',
            'warning': '#F1C40F',
            'info': '#8E44AD'
        }
        
    def load_data_safely(self, file_path, description="ë°ì´í„°"):
        """ì•ˆì „í•œ ë°ì´í„° ë¡œë“œ"""
        try:
            if Path(file_path).exists():
                df = pd.read_csv(file_path)
                print(f"âœ… {description} ë¡œë“œ: {df.shape}")
                return df
            else:
                print(f"âš ï¸ {description} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return None
        except Exception as e:
            print(f"âŒ {description} ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def create_data_distribution_plots(self):
        """ë°ì´í„° ë¶„í¬ ì‹œê°í™”"""
        print("\nğŸ“Š ë°ì´í„° ë¶„í¬ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ì›ë³¸ ë°ì´í„° ë¡œë“œ
        raw_data = self.load_data_safely(self.dataset_path / "0_raw/mimic_mortality_raw.csv", "ì›ë³¸ ë°ì´í„°")
        cleaned_data = self.load_data_safely(self.dataset_path / "1_cleaned/mimic_mortality_cleaned.csv", "ì •ì œ ë°ì´í„°")
        
        if raw_data is None and cleaned_data is None:
            print("âŒ ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('MIMIC-IV 48-hour Mortality Prediction - Data Distribution', fontsize=16, fontweight='bold')
        
        # 1. ì‚¬ë§ë¥  ë¶„í¬ (ì›ë³¸ vs ì •ì œ í›„)
        if raw_data is not None and 'mortality_48h' in raw_data.columns:
            raw_mortality = raw_data['mortality_48h'].mean()
            axes[0,0].bar(['Raw Data'], [raw_mortality], color=self.colors['primary'], alpha=0.7, label='Raw')
        
        if cleaned_data is not None and 'mortality_48h' in cleaned_data.columns:
            cleaned_mortality = cleaned_data['mortality_48h'].mean() 
            axes[0,0].bar(['After Cleaning'], [cleaned_mortality], color=self.colors['success'], alpha=0.7, label='After Cleaning')
        
        axes[0,0].set_ylabel('48-hour Mortality Rate')
        axes[0,0].set_title('Mortality Rate Comparison Before/After Cleaning')
        axes[0,0].legend()
        
        # 2. ì—°ë ¹ ë¶„í¬
        data_for_age = cleaned_data if cleaned_data is not None else raw_data
        if data_for_age is not None:
            age_col = 'anchor_age' if 'anchor_age' in data_for_age.columns else 'age'
            if age_col in data_for_age.columns:
                axes[0,1].hist(data_for_age[age_col].dropna(), bins=30, color=self.colors['secondary'], alpha=0.7, edgecolor='black')
                axes[0,1].set_xlabel('Age')
                axes[0,1].set_ylabel('Frequency')
                axes[0,1].set_title('Patient Age Distribution')
        
        # 3. ì„±ë³„ ë¶„í¬
        if data_for_age is not None and 'gender' in data_for_age.columns:
            gender_counts = data_for_age['gender'].value_counts()
            colors = [self.colors['info'], self.colors['warning']]
            axes[1,0].pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%', 
                         colors=colors, startangle=90)
            axes[1,0].set_title('Gender Distribution')
        
        # 4. ICU ìœ í˜• ë¶„í¬
        if data_for_age is not None and 'first_careunit' in data_for_age.columns:
            icu_counts = data_for_age['first_careunit'].value_counts().head(6)
            axes[1,1].barh(range(len(icu_counts)), icu_counts.values, color=self.colors['primary'])
            axes[1,1].set_yticks(range(len(icu_counts)))
            axes[1,1].set_yticklabels([label[:15] + '...' if len(label) > 15 else label for label in icu_counts.index])
            axes[1,1].set_xlabel('Number of Patients')
            axes[1,1].set_title('ICU Type Distribution (Top 6)')
        
        plt.tight_layout()
        plt.savefig(self.output_path / "01_data_distribution.png", dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… ë°ì´í„° ë¶„í¬ ì‹œê°í™” ì €ì¥")
    
    def create_missing_data_heatmap(self):
        """ê²°ì¸¡ì¹˜ ë¶„ì„ íˆíŠ¸ë§µ"""
        print("\nğŸ“Š ê²°ì¸¡ì¹˜ ë¶„ì„ íˆíŠ¸ë§µ ìƒì„± ì¤‘...")
        
        raw_data = self.load_data_safely(self.dataset_path / "0_raw/mimic_mortality_raw.csv", "ì›ë³¸ ë°ì´í„°")
        
        if raw_data is None:
            print("âŒ ê²°ì¸¡ì¹˜ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê³„ì‚°
        missing_ratios = raw_data.isnull().sum() / len(raw_data)
        missing_ratios = missing_ratios[missing_ratios > 0].sort_values(ascending=False)
        
        if len(missing_ratios) == 0:
            print("âœ… ê²°ì¸¡ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìƒìœ„ 30ê°œ ë³€ìˆ˜ë§Œ í‘œì‹œ
        top_missing = missing_ratios.head(30)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        fig.suptitle('Missing Values Analysis', fontsize=16, fontweight='bold')
        
        # 1. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ë§‰ëŒ€ê·¸ë˜í”„
        y_pos = np.arange(len(top_missing))
        bars = ax1.barh(y_pos, top_missing.values, color=self.colors['danger'], alpha=0.7)
        ax1.set_yticks(y_pos)
        ax1.set_yticklabels(top_missing.index, fontsize=8)
        ax1.set_xlabel('Missing Value Ratio')
        ax1.set_title('Missing Value Ratio by Variable (Top 30)')
        ax1.axvline(x=0.5, color='red', linestyle='--', alpha=0.8, label='50% Threshold')
        ax1.legend()
        
        # ë§‰ëŒ€ì— ê°’ í‘œì‹œ
        for i, bar in enumerate(bars):
            width = bar.get_width()
            ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{width:.1%}', ha='left', va='center', fontsize=7)
        
        # 2. ê²°ì¸¡ì¹˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        ax2.hist(missing_ratios.values, bins=20, color=self.colors['warning'], alpha=0.7, edgecolor='black')
        ax2.axvline(x=0.5, color='red', linestyle='--', alpha=0.8, label='50% Threshold')
        ax2.set_xlabel('Missing Value Ratio')
        ax2.set_ylabel('Number of Variables')
        ax2.set_title('Missing Value Ratio Distribution')
        ax2.legend()
        
        plt.tight_layout()
        plt.savefig(self.output_path / "02_missing_data_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… ê²°ì¸¡ì¹˜ ë¶„ì„ íˆíŠ¸ë§µ ì €ì¥")
        
        # ì¶”ê°€: ê²°ì¸¡ì¹˜ ì œê±° ì „í›„ ë¹„êµ ì‹œê°í™”
        self.create_missing_data_impact_visualization()
    
    def create_missing_data_impact_visualization(self):
        """ê²°ì¸¡ì¹˜ ì œê±° ì „í›„ ë°ì´í„° ê±´ìˆ˜ ë° í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„êµ"""
        print("\nğŸ“Š ê²°ì¸¡ì¹˜ ì œê±° ì˜í–¥ ë¶„ì„ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ì›ë³¸ ë°ì´í„°ì™€ ì •ì œ ë°ì´í„° ë¡œë“œ
        raw_data = self.load_data_safely(self.dataset_path / "0_raw/mimic_mortality_raw.csv", "ì›ë³¸ ë°ì´í„°")
        cleaned_data = self.load_data_safely(self.dataset_path / "1_cleaned/mimic_mortality_cleaned.csv", "ì •ì œ ë°ì´í„°")
        
        if raw_data is None or cleaned_data is None:
            print("âŒ ê²°ì¸¡ì¹˜ ì˜í–¥ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2x2 ë ˆì´ì•„ì›ƒìœ¼ë¡œ ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Missing Data Removal Impact Analysis', fontsize=16, fontweight='bold')
        
        # ë°ì´í„° í†µê³„ ìˆ˜ì§‘
        impact_stats = self.collect_missing_data_impact_stats(raw_data, cleaned_data)
        
        # 1. ë°ì´í„° ê±´ìˆ˜ ë³€í™” (Sankey diagram ìŠ¤íƒ€ì¼)
        self.plot_data_reduction_flow(axes[0, 0], impact_stats)
        
        # 2. í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³€í™”
        self.plot_class_imbalance_change(axes[0, 1], impact_stats)
        
        # 3. ì‚¬ë§ë¥  ë³€í™” ë° í†µê³„
        self.plot_mortality_rate_change(axes[1, 0], impact_stats)
        
        # 4. ì œê±°ëœ ë°ì´í„°ì˜ íŠ¹ì„± ë¶„ì„
        self.plot_removed_data_analysis(axes[1, 1], impact_stats)
        
        plt.tight_layout()
        plt.savefig(self.output_path / "02_missing_data_impact.png", dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… ê²°ì¸¡ì¹˜ ì œê±° ì˜í–¥ ë¶„ì„ ì‹œê°í™” ì €ì¥")
    
    def collect_missing_data_impact_stats(self, raw_data, cleaned_data):
        """ê²°ì¸¡ì¹˜ ì œê±° ì˜í–¥ í†µê³„ ìˆ˜ì§‘"""
        stats = {}
        
        # ê¸°ë³¸ í†µê³„
        stats['raw_count'] = len(raw_data)
        stats['cleaned_count'] = len(cleaned_data)
        stats['removed_count'] = stats['raw_count'] - stats['cleaned_count']
        stats['removal_rate'] = stats['removed_count'] / stats['raw_count']
        
        # ì‚¬ë§ë¥  ê³„ì‚°
        if 'mortality_48h' in raw_data.columns:
            stats['raw_mortality_rate'] = raw_data['mortality_48h'].mean()
            stats['raw_mortality_count'] = raw_data['mortality_48h'].sum()
        else:
            stats['raw_mortality_rate'] = 0
            stats['raw_mortality_count'] = 0
            
        if 'mortality_48h' in cleaned_data.columns:
            stats['cleaned_mortality_rate'] = cleaned_data['mortality_48h'].mean()
            stats['cleaned_mortality_count'] = cleaned_data['mortality_48h'].sum()
        else:
            stats['cleaned_mortality_rate'] = 0
            stats['cleaned_mortality_count'] = 0
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°
        if stats['raw_mortality_count'] > 0:
            stats['raw_imbalance_ratio'] = (stats['raw_count'] - stats['raw_mortality_count']) / stats['raw_mortality_count']
        else:
            stats['raw_imbalance_ratio'] = 0
            
        if stats['cleaned_mortality_count'] > 0:
            stats['cleaned_imbalance_ratio'] = (stats['cleaned_count'] - stats['cleaned_mortality_count']) / stats['cleaned_mortality_count']
        else:
            stats['cleaned_imbalance_ratio'] = 0
        
        # ë¡œê·¸ ì¶œë ¥
        print(f"  ì›ë³¸ ë°ì´í„°: {stats['raw_count']:,}ëª… (ì‚¬ë§ë¥ : {stats['raw_mortality_rate']:.1%})")
        print(f"  ì •ì œ ë°ì´í„°: {stats['cleaned_count']:,}ëª… (ì‚¬ë§ë¥ : {stats['cleaned_mortality_rate']:.1%})")
        print(f"  ì œê±°ëœ ë°ì´í„°: {stats['removed_count']:,}ëª… ({stats['removal_rate']:.1%})")
        
        return stats
    
    def plot_data_reduction_flow(self, ax, stats):
        """ë°ì´í„° ê°ì†Œ í”Œë¡œìš° ì°¨íŠ¸"""
        # ë‹¨ê³„ë³„ ë°ì´í„°
        stages = ['Before\nMissing Removal', 'After\nMissing Removal', 'Removed\nData']
        values = [stats['raw_count'], stats['cleaned_count'], stats['removed_count']]
        colors = [self.colors['primary'], self.colors['success'], self.colors['danger']]
        
        bars = ax.bar(stages, values, color=colors, alpha=0.7)
        
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2, height + max(values)*0.01,
                   f'{value:,}', ha='center', va='bottom', fontweight='bold')
            
            # ë¹„ìœ¨ í‘œì‹œ
            if bar == bars[1]:  # After
                retention_rate = value / stats['raw_count'] * 100
                ax.text(bar.get_x() + bar.get_width()/2, height/2,
                       f'{retention_rate:.1f}%\nretained', ha='center', va='center', 
                       fontweight='bold', color='white', fontsize=10)
            elif bar == bars[2]:  # Removed
                removal_rate = value / stats['raw_count'] * 100
                ax.text(bar.get_x() + bar.get_width()/2, height/2,
                       f'{removal_rate:.1f}%\nremoved', ha='center', va='center', 
                       fontweight='bold', color='white', fontsize=10)
        
        ax.set_ylabel('Number of Patients')
        ax.set_title('Data Reduction by Missing Value Removal', fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')
    
    def plot_class_imbalance_change(self, ax, stats):
        """í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³€í™”"""
        categories = ['Before\nRemoval', 'After\nRemoval']
        imbalance_ratios = [stats['raw_imbalance_ratio'], stats['cleaned_imbalance_ratio']]
        
        bars = ax.bar(categories, imbalance_ratios, 
                     color=[self.colors['warning'], self.colors['info']], alpha=0.7)
        
        # ê°’ í‘œì‹œ
        for bar, ratio in zip(bars, imbalance_ratios):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(imbalance_ratios)*0.01,
                   f'{ratio:.1f}:1', ha='center', va='bottom', fontweight='bold')
        
        # ê°œì„ /ì•…í™” í‘œì‹œ
        if len(imbalance_ratios) >= 2:
            change = imbalance_ratios[1] - imbalance_ratios[0]
            change_text = "Better" if change < 0 else "Worse"
            change_color = "green" if change < 0 else "red"
            ax.text(0.5, max(imbalance_ratios) * 0.8, f'{change_text}\n({change:+.1f})', 
                   ha='center', va='center', fontweight='bold', color=change_color,
                   transform=ax.transData)
        
        ax.set_ylabel('Imbalance Ratio (Survived:Died)')
        ax.set_title('Class Imbalance Change', fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')
    
    def plot_mortality_rate_change(self, ax, stats):
        """ì‚¬ë§ë¥  ë³€í™”"""
        categories = ['Before Removal', 'After Removal']
        mortality_rates = [stats['raw_mortality_rate'] * 100, stats['cleaned_mortality_rate'] * 100]
        
        bars = ax.bar(categories, mortality_rates,
                     color=[self.colors['secondary'], self.colors['primary']], alpha=0.7)
        
        # ê°’ í‘œì‹œ
        for bar, rate in zip(bars, mortality_rates):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(mortality_rates)*0.01,
                   f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # ë³€í™”ëŸ‰ í‘œì‹œ
        if len(mortality_rates) >= 2:
            change = mortality_rates[1] - mortality_rates[0]
            ax.text(0.5, max(mortality_rates) * 0.5, f'{change:+.1f}%p', 
                   ha='center', va='center', fontweight='bold', 
                   color='red' if change > 0 else 'green', fontsize=12)
        
        ax.set_ylabel('Mortality Rate (%)')
        ax.set_title('48-hour Mortality Rate Change', fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')
    
    def plot_removed_data_analysis(self, ax, stats):
        """ì œê±°ëœ ë°ì´í„° ë¶„ì„ (íŒŒì´ ì°¨íŠ¸)"""
        # ì œê±°ëœ ë°ì´í„°ì™€ ë³´ì¡´ëœ ë°ì´í„° ë¹„ìœ¨
        labels = ['Retained Data', 'Removed Data']
        sizes = [stats['cleaned_count'], stats['removed_count']]
        colors = [self.colors['success'], self.colors['danger']]
        
        wedges, texts, autotexts = ax.pie(sizes, labels=labels, autopct='%1.1f%%',
                                         colors=colors, startangle=90, 
                                         explode=(0, 0.1))  # ì œê±°ëœ ë¶€ë¶„ ê°•ì¡°
        
        # ê°œìˆ˜ë„ í‘œì‹œ
        for autotext, size in zip(autotexts, sizes):
            autotext.set_text(f'{size:,}\n({autotext.get_text()})')
            autotext.set_fontsize(9)
            autotext.set_fontweight('bold')
        
        # ì œëª©ê³¼ ì¶”ê°€ ì •ë³´
        ax.set_title('Data Retention vs Removal', fontweight='bold')
        
        # í…ìŠ¤íŠ¸ ë°•ìŠ¤ë¡œ ì¶”ê°€ ì •ë³´
        info_text = f"Total Original: {stats['raw_count']:,}\nRemoval Rate: {stats['removal_rate']:.1%}"
        ax.text(1.2, 0.5, info_text, transform=ax.transAxes, 
               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5),
               fontsize=10, ha='left', va='center')
    
    def create_resampling_comparison(self):
        """ë¦¬ìƒ˜í”Œë§ íš¨ê³¼ ë¹„êµ"""
        print("\nğŸ“Š ë¦¬ìƒ˜í”Œë§ íš¨ê³¼ ë¹„êµ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ë¶„í• ëœ ì›ë³¸ ë°ì´í„°ì™€ ë¦¬ìƒ˜í”Œë§ ë°ì´í„° ë¡œë“œ
        original_train = self.load_data_safely(self.dataset_path / "2_split/mimic_mortality_train.csv", "ì›ë³¸ Train")
        smote_train = self.load_data_safely(self.dataset_path / "3_resampled/smote/mimic_mortality_train.csv", "SMOTE Train")
        down_train = self.load_data_safely(self.dataset_path / "3_resampled/downsampling/mimic_mortality_train.csv", "Downsampling Train")
        
        datasets = [
            ("Original", original_train, self.colors['primary']),
            ("SMOTE", smote_train, self.colors['success']),
            ("Downsampling", down_train, self.colors['warning'])
        ]
        
        available_datasets = [(name, data, color) for name, data, color in datasets if data is not None]
        
        if len(available_datasets) == 0:
            print("âŒ ë¦¬ìƒ˜í”Œë§ ë¹„êµë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Resampling Effect Comparison', fontsize=16, fontweight='bold')
        
        # 1. ë°ì´í„° í¬ê¸° ë¹„êµ
        sizes = []
        names = []
        colors = []
        
        for name, data, color in available_datasets:
            if data is not None:
                sizes.append(len(data))
                names.append(name)
                colors.append(color)
        
        bars = axes[0,0].bar(names, sizes, color=colors, alpha=0.7)
        axes[0,0].set_ylabel('Number of Samples')
        axes[0,0].set_title('Dataset Size Comparison')
        
        # ë§‰ëŒ€ì— ê°’ í‘œì‹œ
        for bar, size in zip(bars, sizes):
            axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(sizes)*0.01,
                          f'{size:,}', ha='center', va='bottom', fontweight='bold')
        
        # 2. í´ë˜ìŠ¤ ë¶„í¬ ë¹„êµ (ë¹„ìœ¨)
        ratios = []
        for name, data, color in available_datasets:
            if data is not None and 'mortality_48h' in data.columns:
                mortality_rate = data['mortality_48h'].mean()
                ratios.append([1-mortality_rate, mortality_rate])
            else:
                ratios.append([0, 0])
        
        if ratios:
            ratios = np.array(ratios)
            x = np.arange(len(names))
            width = 0.35
            
            axes[0,1].bar(x, ratios[:, 0], width, label='Survived (0)', color=self.colors['info'], alpha=0.7)
            axes[0,1].bar(x, ratios[:, 1], width, bottom=ratios[:, 0], label='Died (1)', color=self.colors['danger'], alpha=0.7)
            
            axes[0,1].set_ylabel('Ratio')
            axes[0,1].set_title('Class Distribution Comparison')
            axes[0,1].set_xticks(x)
            axes[0,1].set_xticklabels(names)
            axes[0,1].legend()
        
        # 3. ë¶ˆê· í˜• ë¹„ìœ¨ ë¹„êµ
        imbalance_ratios = []
        for name, data, color in available_datasets:
            if data is not None and 'mortality_48h' in data.columns:
                counts = data['mortality_48h'].value_counts()
                if len(counts) >= 2:
                    ratio = counts[0] / counts[1]
                    imbalance_ratios.append(ratio)
                else:
                    imbalance_ratios.append(0)
            else:
                imbalance_ratios.append(0)
        
        bars = axes[1,0].bar(names, imbalance_ratios, color=colors, alpha=0.7)
        axes[1,0].axhline(y=1, color='red', linestyle='--', alpha=0.8, label='Perfect Balance')
        axes[1,0].set_ylabel('Imbalance Ratio (Survived:Died)')
        axes[1,0].set_title('Class Imbalance Ratio')
        axes[1,0].legend()
        
        # ë§‰ëŒ€ì— ê°’ í‘œì‹œ
        for bar, ratio in zip(bars, imbalance_ratios):
            axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(imbalance_ratios)*0.02,
                          f'{ratio:.1f}:1', ha='center', va='bottom', fontweight='bold')
        
        # 4. ì‚¬ë§ë¥  ë¹„êµ
        mortality_rates = []
        for name, data, color in available_datasets:
            if data is not None and 'mortality_48h' in data.columns:
                mortality_rate = data['mortality_48h'].mean()
                mortality_rates.append(mortality_rate)
            else:
                mortality_rates.append(0)
        
        bars = axes[1,1].bar(names, mortality_rates, color=colors, alpha=0.7)
        axes[1,1].set_ylabel('Mortality Rate')
        axes[1,1].set_title('48-hour Mortality Rate Comparison')
        
        # ë§‰ëŒ€ì— ê°’ í‘œì‹œ
        for bar, rate in zip(bars, mortality_rates):
            axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(mortality_rates)*0.01,
                          f'{rate:.1%}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.output_path / "03_resampling_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… ë¦¬ìƒ˜í”Œë§ íš¨ê³¼ ë¹„êµ ì €ì¥")
        
        # ì¶”ê°€: ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í¬ê¸° ë³€í™” ì‹œê°í™”
        self.create_data_pipeline_visualization()
    
    def create_data_pipeline_visualization(self):
        """ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ í¬ê¸° ë³€í™” ì‹œê°í™”"""
        print("\nğŸ“Š ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ë°ì´í„° í¬ê¸° ìˆ˜ì§‘
        pipeline_data = self.collect_pipeline_data()
        
        if not pipeline_data:
            print("âŒ íŒŒì´í”„ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return
        
        # 2x2 ë ˆì´ì•„ì›ƒìœ¼ë¡œ ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Data Processing Pipeline - Dataset Size Changes', fontsize=16, fontweight='bold')
        
        # 1. ì „ì²´ íŒŒì´í”„ë¼ì¸ í”Œë¡œìš°
        self.plot_pipeline_flow(axes[0, 0], pipeline_data)
        
        # 2. ë¶„í•  ì „í›„ ë¹„êµ
        self.plot_split_comparison(axes[0, 1], pipeline_data)
        
        # 3. ë¦¬ìƒ˜í”Œë§ íš¨ê³¼ (Train ì„¸íŠ¸)
        self.plot_resampling_effects(axes[1, 0], pipeline_data)
        
        # 4. ìµœì¢… ë°ì´í„°ì…‹ ë¶„í¬ (íŒŒì´ ì°¨íŠ¸)
        self.plot_final_distribution(axes[1, 1], pipeline_data)
        
        plt.tight_layout()
        plt.savefig(self.output_path / "03_data_pipeline.png", dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œê°í™” ì €ì¥")
    
    def collect_pipeline_data(self):
        """íŒŒì´í”„ë¼ì¸ ê° ë‹¨ê³„ì˜ ë°ì´í„° í¬ê¸° ìˆ˜ì§‘"""
        pipeline_data = {}
        
        # ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì •ì˜
        data_files = {
            'raw': self.dataset_path / "0_raw/mimic_mortality_raw.csv",
            'cleaned': self.dataset_path / "1_cleaned/mimic_mortality_cleaned.csv", 
            'train_original': self.dataset_path / "2_split/mimic_mortality_train.csv",
            'val': self.dataset_path / "2_split/mimic_mortality_validation.csv",
            'test': self.dataset_path / "2_split/mimic_mortality_test.csv",
            'train_smote': self.dataset_path / "3_resampled/smote/mimic_mortality_train.csv",
            'train_downsampling': self.dataset_path / "3_resampled/downsampling/mimic_mortality_train.csv"
        }
        
        # ê° íŒŒì¼ì˜ í¬ê¸° ë° ì‚¬ë§ë¥  ìˆ˜ì§‘
        for stage, file_path in data_files.items():
            data = self.load_data_safely(file_path, f"{stage} ë°ì´í„°")
            if data is not None:
                size = len(data)
                mortality_rate = data['mortality_48h'].mean() if 'mortality_48h' in data.columns else 0
                mortality_count = data['mortality_48h'].sum() if 'mortality_48h' in data.columns else 0
                
                pipeline_data[stage] = {
                    'size': size,
                    'mortality_rate': mortality_rate,
                    'mortality_count': int(mortality_count),
                    'survival_count': int(size - mortality_count)
                }
                print(f"  {stage}: {size:,}ëª… (ì‚¬ë§ë¥ : {mortality_rate:.1%})")
            else:
                pipeline_data[stage] = None
        
        return pipeline_data
    
    def plot_pipeline_flow(self, ax, pipeline_data):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ í”Œë¡œìš° ì°¨íŠ¸"""
        stages = ['raw', 'cleaned', 'train_original', 'train_smote', 'train_downsampling']
        stage_names = ['Raw Data', 'Cleaned', 'Train Split', 'SMOTE Train', 'Downsample Train']
        
        sizes = []
        colors = []
        
        for stage in stages:
            if pipeline_data.get(stage):
                sizes.append(pipeline_data[stage]['size'])
                if 'raw' in stage:
                    colors.append(self.colors['primary'])
                elif 'cleaned' in stage:
                    colors.append(self.colors['secondary'])
                elif 'original' in stage:
                    colors.append(self.colors['info'])
                elif 'smote' in stage:
                    colors.append(self.colors['success'])
                else:
                    colors.append(self.colors['warning'])
            else:
                sizes.append(0)
                colors.append('gray')
        
        # ë§‰ëŒ€ ê·¸ë˜í”„
        bars = ax.bar(range(len(stage_names)), sizes, color=colors, alpha=0.7)
        
        # ê°’ í‘œì‹œ
        for i, (bar, size) in enumerate(zip(bars, sizes)):
            if size > 0:
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(sizes)*0.01,
                       f'{size:,}', ha='center', va='bottom', fontweight='bold', fontsize=9)
        
        ax.set_xticks(range(len(stage_names)))
        ax.set_xticklabels(stage_names, rotation=45, ha='right')
        ax.set_ylabel('Number of Samples')
        ax.set_title('Data Pipeline Flow', fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')
    
    def plot_split_comparison(self, ax, pipeline_data):
        """ë¶„í•  ì „í›„ ë¹„êµ"""
        # ë¶„í•  ì „ (cleaned) vs ë¶„í•  í›„ (train+val+test)
        datasets = {}
        
        if pipeline_data.get('cleaned'):
            datasets['Before Split\n(Cleaned)'] = pipeline_data['cleaned']['size']
        
        # ë¶„í•  í›„ ì´í•©
        split_total = 0
        for stage in ['train_original', 'val', 'test']:
            if pipeline_data.get(stage):
                split_total += pipeline_data[stage]['size']
        
        if split_total > 0:
            datasets['After Split\n(Train+Val+Test)'] = split_total
        
        if datasets:
            names = list(datasets.keys())
            sizes = list(datasets.values())
            colors = [self.colors['secondary'], self.colors['info']]
            
            bars = ax.bar(names, sizes, color=colors, alpha=0.7)
            
            # ê°’ í‘œì‹œ
            for bar, size in zip(bars, sizes):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(sizes)*0.01,
                       f'{size:,}', ha='center', va='bottom', fontweight='bold')
            
            ax.set_ylabel('Number of Samples')
            ax.set_title('Before/After Data Split', fontweight='bold')
            ax.grid(True, alpha=0.3, axis='y')
    
    def plot_resampling_effects(self, ax, pipeline_data):
        """ë¦¬ìƒ˜í”Œë§ íš¨ê³¼ (Train ì„¸íŠ¸ í¬ê¸° ë³€í™”)"""
        train_datasets = {}
        colors = []
        
        if pipeline_data.get('train_original'):
            train_datasets['Original\nTrain'] = pipeline_data['train_original']['size']
            colors.append(self.colors['primary'])
        
        if pipeline_data.get('train_smote'):
            train_datasets['SMOTE\nTrain'] = pipeline_data['train_smote']['size']
            colors.append(self.colors['success'])
        
        if pipeline_data.get('train_downsampling'):
            train_datasets['Downsampling\nTrain'] = pipeline_data['train_downsampling']['size']
            colors.append(self.colors['warning'])
        
        if train_datasets:
            names = list(train_datasets.keys())
            sizes = list(train_datasets.values())
            
            bars = ax.bar(names, sizes, color=colors, alpha=0.7)
            
            # ê°’ í‘œì‹œ ë° ë³€í™”ìœ¨ ê³„ì‚°
            original_size = train_datasets.get('Original\nTrain', 0)
            for i, (bar, size) in enumerate(zip(bars, sizes)):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(sizes)*0.01,
                       f'{size:,}', ha='center', va='bottom', fontweight='bold', fontsize=9)
                
                # ë³€í™”ìœ¨ í‘œì‹œ (ì›ë³¸ ëŒ€ë¹„)
                if original_size > 0 and i > 0:
                    change_pct = (size - original_size) / original_size * 100
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,
                           f'{change_pct:+.0f}%', ha='center', va='center', 
                           fontweight='bold', color='white', fontsize=8)
            
            ax.set_ylabel('Number of Training Samples')
            ax.set_title('Resampling Effects on Training Set', fontweight='bold')
            ax.grid(True, alpha=0.3, axis='y')
    
    def plot_final_distribution(self, ax, pipeline_data):
        """ìµœì¢… ë°ì´í„°ì…‹ ë¶„í¬ (Val, Test, SMOTE Train, Downsample Train)"""
        final_datasets = {}
        
        # Val, TestëŠ” ê³ ì •
        if pipeline_data.get('val'):
            final_datasets['Validation'] = pipeline_data['val']['size']
        if pipeline_data.get('test'):
            final_datasets['Test'] = pipeline_data['test']['size']
        
        # ê°€ì¥ í° Train ì„¸íŠ¸ ì„ íƒ (ë³´í†µ SMOTE)
        max_train_size = 0
        max_train_name = ''
        for train_type in ['train_smote', 'train_downsampling', 'train_original']:
            if pipeline_data.get(train_type):
                size = pipeline_data[train_type]['size']
                if size > max_train_size:
                    max_train_size = size
                    max_train_name = train_type.replace('train_', '').replace('_', ' ').title()
        
        if max_train_size > 0:
            final_datasets[f'{max_train_name} Train'] = max_train_size
        
        if final_datasets:
            labels = list(final_datasets.keys())
            sizes = list(final_datasets.values())
            colors = [self.colors['success'], self.colors['info'], self.colors['warning']][:len(sizes)]
            
            wedges, texts, autotexts = ax.pie(sizes, labels=labels, autopct='%1.1f%%', 
                                             colors=colors, startangle=90)
            
            # ê°œìˆ˜ë„ í‘œì‹œ
            for i, (autotext, size) in enumerate(zip(autotexts, sizes)):
                autotext.set_text(f'{size:,}\n({autotext.get_text()})')
                autotext.set_fontsize(8)
            
            ax.set_title('Final Dataset Distribution', fontweight='bold')
    
    def create_model_performance_plots(self):
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"""
        print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ëª¨ë¸ë§ ê²°ê³¼ ë¡œë“œ
        modeling_results = self.load_data_safely(self.dataset_path / "4_modeling/modeling_results.csv", "ëª¨ë¸ë§ ê²°ê³¼")
        tuning_results = self.load_data_safely(self.dataset_path / "5_final_models/tuning_results.csv", "íŠœë‹ ê²°ê³¼")
        
        if modeling_results is None:
            print("âŒ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')
        
        # 1. ëª¨ë¸ë³„ ROC-AUC ë¹„êµ
        model_auc = modeling_results.groupby('model_name')['roc_auc'].mean().sort_values(ascending=True)
        
        bars = axes[0,0].barh(range(len(model_auc)), model_auc.values, color=self.colors['primary'], alpha=0.7)
        axes[0,0].set_yticks(range(len(model_auc)))
        axes[0,0].set_yticklabels(model_auc.index)
        axes[0,0].set_xlabel('ROC-AUC')
        axes[0,0].set_title('Average ROC-AUC by Model')
        
        # ê°’ í‘œì‹œ
        for i, (bar, value) in enumerate(zip(bars, model_auc.values)):
            axes[0,0].text(value + 0.005, bar.get_y() + bar.get_height()/2,
                          f'{value:.3f}', va='center', fontweight='bold')
        
        # 2. ë¦¬ìƒ˜í”Œë§ë³„ ì„±ëŠ¥ ë¹„êµ
        resampling_auc = modeling_results.groupby('resampling_method')['roc_auc'].mean()
        
        colors = [self.colors['success'], self.colors['warning']]
        bars = axes[0,1].bar(resampling_auc.index, resampling_auc.values, color=colors, alpha=0.7)
        axes[0,1].set_ylabel('Average ROC-AUC')
        axes[0,1].set_title('Performance by Resampling Method')
        
        for bar, value in zip(bars, resampling_auc.values):
            axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                          f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 3. ìƒìœ„ 10ê°œ ëª¨ë¸ ì„±ëŠ¥
        top_10 = modeling_results.nlargest(10, 'roc_auc')
        
        x_pos = np.arange(len(top_10))
        bars = axes[1,0].bar(x_pos, top_10['roc_auc'], color=self.colors['info'], alpha=0.7)
        axes[1,0].set_xticks(x_pos)
        axes[1,0].set_xticklabels([f"{row['resampling_method'][:4]}\n{row['model_name'][:8]}" 
                                  for _, row in top_10.iterrows()], rotation=45, fontsize=8)
        axes[1,0].set_ylabel('ROC-AUC')
        axes[1,0].set_title('Top 10 Model Performance')
        
        # 4. íŠœë‹ ì „í›„ ë¹„êµ (ìˆëŠ” ê²½ìš°)
        if tuning_results is not None:
            # íŠœë‹ëœ ëª¨ë¸ë“¤ê³¼ ì›ë˜ ëª¨ë¸ ë¹„êµ
            axes[1,1].set_title('Hyperparameter Tuning Effect')
            
            tuning_models = tuning_results['model_name'].unique()
            before_after = []
            
            for model in tuning_models:
                # ì›ë˜ ì„±ëŠ¥
                original_perf = modeling_results[modeling_results['model_name'] == model]['roc_auc'].max()
                # íŠœë‹ í›„ ì„±ëŠ¥
                tuned_perf = tuning_results[tuning_results['model_name'] == model]['roc_auc'].iloc[0]
                
                before_after.append({
                    'model': model,
                    'before': original_perf,
                    'after': tuned_perf,
                    'improvement': tuned_perf - original_perf
                })
            
            if before_after:
                models = [item['model'] for item in before_after]
                before_values = [item['before'] for item in before_after]
                after_values = [item['after'] for item in before_after]
                
                x = np.arange(len(models))
                width = 0.35
                
                axes[1,1].bar(x - width/2, before_values, width, label='Before Tuning', alpha=0.7)
                axes[1,1].bar(x + width/2, after_values, width, label='After Tuning', alpha=0.7)
                
                axes[1,1].set_ylabel('ROC-AUC')
                axes[1,1].set_xticks(x)
                axes[1,1].set_xticklabels(models, rotation=45)
                axes[1,1].legend()
        else:
            # íŠœë‹ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ F1-Score ë¹„êµ
            model_f1 = modeling_results.groupby('model_name')['f1_score'].mean().sort_values(ascending=False)
            
            bars = axes[1,1].bar(range(len(model_f1)), model_f1.values, color=self.colors['secondary'], alpha=0.7)
            axes[1,1].set_xticks(range(len(model_f1)))
            axes[1,1].set_xticklabels(model_f1.index, rotation=45)
            axes[1,1].set_ylabel('F1-Score')
            axes[1,1].set_title('Average F1-Score by Model')
        
        plt.tight_layout()
        plt.savefig(self.output_path / "04_model_performance.png", dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ì €ì¥")
    
    def create_final_dashboard(self):
        """ìµœì¢… ê²°ê³¼ ëŒ€ì‹œë³´ë“œ"""
        print("\nğŸ“Š ìµœì¢… ê²°ê³¼ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")
        
        # ëª¨ë“  ì£¼ìš” ê²°ê³¼ ë¡œë“œ
        modeling_results = self.load_data_safely(self.dataset_path / "4_modeling/modeling_results.csv", "ëª¨ë¸ë§ ê²°ê³¼")
        tuning_results = self.load_data_safely(self.dataset_path / "5_final_models/tuning_results.csv", "íŠœë‹ ê²°ê³¼")
        
        # ëŒ€ì‹œë³´ë“œ ìƒì„±
        fig = plt.figure(figsize=(20, 12))
        fig.suptitle('MIMIC-IV 48-hour Mortality Prediction - Final Results Dashboard', fontsize=20, fontweight='bold')
        
        # GridSpecì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ì•„ì›ƒ ì„¤ì •
        gs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1, 1])
        
        # 1. í”„ë¡œì íŠ¸ ê°œìš” (í…ìŠ¤íŠ¸)
        ax1 = fig.add_subplot(gs[0, :2])
        ax1.axis('off')
        
        project_info = """
ğŸ¥ MIMIC-IV ICU 48ì‹œê°„ ì‚¬ë§ë¥  ì˜ˆì¸¡ í”„ë¡œì íŠ¸

ğŸ“Š ë°ì´í„°: MIMIC-IV ì¤‘í™˜ìì‹¤ ë°ì´í„°
ğŸ¯ ëª©í‘œ: ICU ì…ì‹¤ í›„ 24ì‹œê°„ ë°ì´í„°ë¡œ 48ì‹œê°„ ì‚¬ë§ ì˜ˆì¸¡
â° ì‹œê°„ ë²”ìœ„: ICU ì…ì‹¤ í›„ 24ì‹œê°„ ë‚´ ìƒì²´ì§•í›„/ê²€ì‚¬ìˆ˜ì¹˜
ğŸ§¬ íŠ¹ì„±: ìƒì²´ì§•í›„, ê²€ì‚¬ìˆ˜ì¹˜, ì¸êµ¬í•™ì  ì •ë³´, ë™ë°˜ì§ˆí™˜

ğŸ”¬ ëª¨ë¸: 6ê°œ (Logistic Regression, SVC, Random Forest, XGBoost, LightGBM, Extra Trees)
âš–ï¸ ë¦¬ìƒ˜í”Œë§: SMOTE, Downsampling
ğŸ¯ ìµœì í™”: Optuna ë² ì´ì§€ì•ˆ ìµœì í™”
        """
        
        ax1.text(0.05, 0.95, project_info, transform=ax1.transAxes, fontsize=11,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))
        
        # 2. ì£¼ìš” ì§€í‘œ (í…ìŠ¤íŠ¸)
        ax2 = fig.add_subplot(gs[0, 2:])
        ax2.axis('off')
        
        if modeling_results is not None:
            best_model = modeling_results.loc[modeling_results['roc_auc'].idxmax()]
            total_models = len(modeling_results)
            
            metrics_info = f"""
ğŸ“ˆ ì£¼ìš” ì„±ê³¼

ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['resampling_method'].upper()}-{best_model['model_name']}
ğŸ“Š ROC-AUC: {best_model['roc_auc']:.4f}
ğŸ“ F1-Score: {best_model['f1_score']:.4f}
ğŸ¯ Precision: {best_model['precision']:.4f}
ğŸ” Recall: {best_model['recall']:.4f}

ğŸ¤– ì´ ì‹¤í—˜ ëª¨ë¸: {total_models}ê°œ
âœ… ì¬í˜„ ê°€ëŠ¥ì„±: 100% (ì‹œë“œ ê³ ì •)
            """
            
            ax2.text(0.05, 0.95, metrics_info, transform=ax2.transAxes, fontsize=11,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))
        
        # 3. ìƒìœ„ ëª¨ë¸ ì„±ëŠ¥ (ë§‰ëŒ€ ê·¸ë˜í”„)
        if modeling_results is not None:
            ax3 = fig.add_subplot(gs[1, :2])
            top_5 = modeling_results.nlargest(5, 'roc_auc')
            
            bars = ax3.barh(range(len(top_5)), top_5['roc_auc'], color=self.colors['primary'], alpha=0.8)
            ax3.set_yticks(range(len(top_5)))
            ax3.set_yticklabels([f"{row['resampling_method'][:6]}-{row['model_name'][:10]}" 
                               for _, row in top_5.iterrows()])
            ax3.set_xlabel('ROC-AUC')
            ax3.set_title('Top 5 Model Performance', fontweight='bold')
            
            # ê°’ í‘œì‹œ
            for i, (bar, value) in enumerate(zip(bars, top_5['roc_auc'])):
                ax3.text(value + 0.005, bar.get_y() + bar.get_height()/2,
                        f'{value:.3f}', va='center', fontweight='bold')
        
        # 4. ë¦¬ìƒ˜í”Œë§ íš¨ê³¼ (íŒŒì´ ì°¨íŠ¸)
        if modeling_results is not None:
            ax4 = fig.add_subplot(gs[1, 2:])
            resampling_perf = modeling_results.groupby('resampling_method')['roc_auc'].mean()
            
            colors = [self.colors['success'], self.colors['warning']]
            wedges, texts, autotexts = ax4.pie(resampling_perf.values, 
                                              labels=resampling_perf.index,
                                              autopct='%1.3f', 
                                              colors=colors,
                                              startangle=90)
            ax4.set_title('Average Performance by Resampling Method', fontweight='bold')
        
        # 5. ìµœì¢… ê²°ê³¼ ìš”ì•½ í…Œì´ë¸”
        ax5 = fig.add_subplot(gs[2, :])
        ax5.axis('off')
        
        if tuning_results is not None and len(tuning_results) > 0:
            # íŠœë‹ ê²°ê³¼ í…Œì´ë¸”
            table_data = []
            for _, row in tuning_results.iterrows():
                table_data.append([
                    row['model_name'],
                    f"{row['roc_auc']:.4f}",
                    f"{row['f1_score']:.4f}",
                    f"{row['precision']:.4f}",
                    f"{row['recall']:.4f}",
                    f"{row['accuracy']:.4f}"
                ])
            
            table = ax5.table(cellText=table_data,
                            colLabels=['ëª¨ë¸', 'ROC-AUC', 'F1-Score', 'Precision', 'Recall', 'Accuracy'],
                            cellLoc='center',
                            loc='center',
                            cellColours=None)
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1, 2)
            
            # í—¤ë” ìŠ¤íƒ€ì¼ë§
            for i in range(6):
                table[(0, i)].set_facecolor('#4CAF50')
                table[(0, i)].set_text_props(weight='bold', color='white')
            
            ax5.set_title('Final Model Performance (Test Set)', fontweight='bold', pad=20)
        
        elif modeling_results is not None:
            # ëª¨ë¸ë§ ê²°ê³¼ë§Œ ìˆëŠ” ê²½ìš°
            table_data = []
            for _, row in modeling_results.head(3).iterrows():
                table_data.append([
                    f"{row['resampling_method']}-{row['model_name']}",
                    f"{row['roc_auc']:.4f}",
                    f"{row['f1_score']:.4f}",
                    f"{row['precision']:.4f}",
                    f"{row['recall']:.4f}",
                    f"{row['accuracy']:.4f}"
                ])
            
            table = ax5.table(cellText=table_data,
                            colLabels=['ëª¨ë¸', 'ROC-AUC', 'F1-Score', 'Precision', 'Recall', 'Accuracy'],
                            cellLoc='center',
                            loc='center')
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1, 2)
            
            ax5.set_title('Top 3 Model Performance (Validation Set)', fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.savefig(self.output_path / "05_final_dashboard.png", dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… ìµœì¢… ê²°ê³¼ ëŒ€ì‹œë³´ë“œ ì €ì¥")
    
    def generate_shap_visualizations(self):
        """SHAP ë¶„ì„ ì‹œê°í™”"""
        print("\nğŸ“Š SHAP ë¶„ì„ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        shap_path = self.base_path / "results" / "07_shap_analysis"
        
        if not shap_path.exists():
            print("âš ï¸ SHAP ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. 07_shap_analysis.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        # SHAP ê²°ê³¼ íŒŒì¼ë“¤ ì°¾ê¸°
        feature_importance_files = list(shap_path.glob("*_feature_importance.csv"))
        shap_value_files = list(shap_path.glob("*_shap_values.json"))
        
        if not feature_importance_files:
            print("âš ï¸ SHAP feature importance íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Figure ìƒì„±
        fig = plt.figure(figsize=(20, 16))
        fig.suptitle('MIMIC-IV SHAP Analysis - Model Interpretability', fontsize=20, fontweight='bold')
        
        # 2x2 ë ˆì´ì•„ì›ƒ
        gs = fig.add_gridspec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1], hspace=0.3, wspace=0.3)
        
        # 1. ëª¨ë¸ë³„ ìƒìœ„ íŠ¹ì„± ë¹„êµ
        ax1 = fig.add_subplot(gs[0, 0])
        self.plot_model_feature_comparison(ax1, feature_importance_files)
        
        # 2. ì „ì²´ ìƒìœ„ íŠ¹ì„± (ëª¨ë“  ëª¨ë¸ í‰ê· )
        ax2 = fig.add_subplot(gs[0, 1])
        self.plot_global_feature_importance(ax2, feature_importance_files)
        
        # 3. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ ìƒìœ„ 20ê°œ íŠ¹ì„±
        ax3 = fig.add_subplot(gs[1, 0])
        self.plot_best_model_features(ax3, feature_importance_files)
        
        # 4. íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„
        ax4 = fig.add_subplot(gs[1, 1])
        self.plot_feature_categories(ax4, feature_importance_files)
        
        # ì €ì¥
        shap_viz_path = self.output_path / "05_shap_analysis.png"
        plt.savefig(shap_viz_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print("âœ… SHAP ë¶„ì„ ì‹œê°í™” ì €ì¥")
        
        # ë³„ë„ë¡œ beeplot ìƒì„±
        self.generate_shap_beeplot()
    
    def plot_model_feature_comparison(self, ax, feature_importance_files):
        """ëª¨ë¸ë³„ ìƒìœ„ íŠ¹ì„± ë¹„êµ"""
        top_features_by_model = {}
        
        # ê° ëª¨ë¸ì˜ ìƒìœ„ 5ê°œ íŠ¹ì„± ì¶”ì¶œ
        for file_path in feature_importance_files:
            model_name = file_path.stem.replace('_feature_importance', '')
            df = pd.read_csv(file_path)
            top_5 = df.head(5)
            top_features_by_model[model_name] = top_5
        
        # ì „ì²´ íŠ¹ì„± ì§‘í•©
        all_features = set()
        for model_features in top_features_by_model.values():
            all_features.update(model_features['feature'].tolist())
        
        # ê° íŠ¹ì„±ë³„ë¡œ ëª¨ë¸ë³„ ì¤‘ìš”ë„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        feature_importance_matrix = []
        feature_names = []
        
        # ìƒìœ„ ë¹ˆë„ íŠ¹ì„±ë“¤ë§Œ ì„ íƒ
        feature_counts = {}
        for model_features in top_features_by_model.values():
            for feature in model_features['feature']:
                feature_counts[feature] = feature_counts.get(feature, 0) + 1
        
        # 2ê°œ ì´ìƒ ëª¨ë¸ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” íŠ¹ì„±ë“¤ë§Œ
        common_features = [f for f, c in feature_counts.items() if c >= 2][:10]
        
        for feature in common_features:
            importances = []
            for model_name, model_features in top_features_by_model.items():
                feature_row = model_features[model_features['feature'] == feature]
                if not feature_row.empty:
                    importances.append(feature_row['importance'].values[0])
                else:
                    importances.append(0)
            feature_importance_matrix.append(importances)
            feature_names.append(feature[:20] + '...' if len(feature) > 20 else feature)
        
        if not feature_importance_matrix:
            ax.text(0.5, 0.5, 'No common features found', ha='center', va='center')
            ax.set_title('Model Feature Comparison')
            return
        
        # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
        feature_importance_matrix = np.array(feature_importance_matrix)
        model_names = [name.replace('_', '\n')[:15] for name in top_features_by_model.keys()]
        
        im = ax.imshow(feature_importance_matrix, cmap='Reds', aspect='auto')
        
        # ì¶• ì„¤ì •
        ax.set_xticks(range(len(model_names)))
        ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=8)
        ax.set_yticks(range(len(feature_names)))
        ax.set_yticklabels(feature_names, fontsize=8)
        ax.set_title('Feature Importance by Model', fontweight='bold')
        
        # ì»¬ëŸ¬ë°”
        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    
    def plot_global_feature_importance(self, ax, feature_importance_files):
        """ì „ì²´ ìƒìœ„ íŠ¹ì„± (ëª¨ë“  ëª¨ë¸ í‰ê· )"""
        all_features = {}
        
        # ëª¨ë“  ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ìˆ˜ì§‘
        for file_path in feature_importance_files:
            df = pd.read_csv(file_path)
            for _, row in df.iterrows():
                feature = row['feature']
                importance = row['importance']
                if feature not in all_features:
                    all_features[feature] = []
                all_features[feature].append(importance)
        
        # í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
        avg_importance = {feature: np.mean(importances) 
                         for feature, importances in all_features.items()}
        
        # ìƒìœ„ 15ê°œ ì„ íƒ
        top_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)[:15]
        
        features, importances = zip(*top_features)
        
        # íŠ¹ì„±ëª… ë‹¨ì¶•
        short_features = [f[:25] + '...' if len(f) > 25 else f for f in features]
        
        # ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„
        bars = ax.barh(range(len(short_features)), importances, color=self.colors['primary'], alpha=0.7)
        
        ax.set_yticks(range(len(short_features)))
        ax.set_yticklabels(short_features, fontsize=9)
        ax.set_xlabel('Average SHAP Importance')
        ax.set_title('Top Features Across All Models', fontweight='bold')
        
        # ê°’ í‘œì‹œ
        for i, bar in enumerate(bars):
            width = bar.get_width()
            ax.text(width + width*0.01, bar.get_y() + bar.get_height()/2, 
                   f'{width:.3f}', ha='left', va='center', fontsize=8)
        
        ax.invert_yaxis()
    
    def plot_best_model_features(self, ax, feature_importance_files):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ ìƒìœ„ íŠ¹ì„±"""
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸° (íŒŒì¼ëª…ì—ì„œ ROC-AUC ì •ë³´ ì—†ìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©)
        best_model_file = feature_importance_files[0]  # ì´ë¯¸ ì„±ëŠ¥ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
        
        df = pd.read_csv(best_model_file)
        top_20 = df.head(20)
        
        model_name = best_model_file.stem.replace('_feature_importance', '')
        
        # íŠ¹ì„±ëª… ë‹¨ì¶•
        short_features = [f[:30] + '...' if len(f) > 30 else f for f in top_20['feature']]
        
        bars = ax.barh(range(len(short_features)), top_20['importance'], 
                      color=self.colors['success'], alpha=0.7)
        
        ax.set_yticks(range(len(short_features)))
        ax.set_yticklabels(short_features, fontsize=8)
        ax.set_xlabel('SHAP Importance')
        ax.set_title(f'Top 20 Features - Best Model\n({model_name.replace("_", " ")})', fontweight='bold')
        
        # ê°’ í‘œì‹œ (ìƒìœ„ 10ê°œë§Œ)
        for i, bar in enumerate(bars[:10]):
            width = bar.get_width()
            ax.text(width + width*0.01, bar.get_y() + bar.get_height()/2, 
                   f'{width:.3f}', ha='left', va='center', fontsize=8)
        
        ax.invert_yaxis()
    
    def plot_feature_categories(self, ax, feature_importance_files):
        """íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„"""
        # íŠ¹ì„± ì¹´í…Œê³ ë¦¬ ì •ì˜
        categories = {
            'vital_signs': ['hr_', 'sbp_', 'dbp_', 'mbp_', 'temp_', 'resp_', 'spo2_'],
            'lab_results': ['glucose_', 'creatinine_', 'bun_', 'sodium_', 'potassium_', 
                           'hemoglobin_', 'hematocrit_', 'platelet_', 'wbc_'],
            'demographics': ['anchor_age', 'gender'],
            'comorbidities': ['diabetes', 'hypertension', 'obesity'],
            'icu_info': ['first_careunit', 'los'],
            'sleep_sedation': ['sleep_', 'sedation_']
        }
        
        # ëª¨ë“  ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ìˆ˜ì§‘
        all_importances = {}
        for file_path in feature_importance_files:
            df = pd.read_csv(file_path)
            for _, row in df.iterrows():
                feature = row['feature']
                importance = row['importance']
                if feature not in all_importances:
                    all_importances[feature] = []
                all_importances[feature].append(importance)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
        category_importance = {}
        for category, keywords in categories.items():
            total_importance = 0
            count = 0
            for feature, importances in all_importances.items():
                if any(keyword in feature.lower() for keyword in keywords):
                    total_importance += np.mean(importances)
                    count += 1
            
            if count > 0:
                category_importance[category] = total_importance / count
            else:
                category_importance[category] = 0
        
        # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
        categories = list(category_importance.keys())
        importances = list(category_importance.values())
        
        bars = ax.bar(categories, importances, color=[
            self.colors['primary'], self.colors['secondary'], self.colors['success'],
            self.colors['danger'], self.colors['warning'], self.colors['info']
        ][:len(categories)], alpha=0.7)
        
        ax.set_ylabel('Average SHAP Importance')
        ax.set_title('Feature Importance by Category', fontweight='bold')
        ax.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    
    def generate_shap_beeplot(self):
        """SHAP Beeplot (SHAP ê°’ ë¶„í¬ ì‹œê°í™”) - F1 ìµœê³  & AUROC ìµœê³  ëª¨ë¸ 2ê°œ"""
        print("\nğŸ“Š SHAP Beeplot ìƒì„± ì¤‘...")
        
        # ëª¨ë¸ë§ ê²°ê³¼ì—ì„œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        modeling_results_path = self.dataset_path / "4_modeling/modeling_results.csv"
        if not modeling_results_path.exists():
            print("âš ï¸ ëª¨ë¸ë§ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        results_df = pd.read_csv(modeling_results_path)
        
        # F1 Score ìµœê³  ëª¨ë¸
        best_f1_row = results_df.loc[results_df['f1_score'].idxmax()]
        best_f1_model = f"{best_f1_row['resampling_method']}_{best_f1_row['model_name']}"
        
        # ROC-AUC ìµœê³  ëª¨ë¸ 
        best_auc_row = results_df.loc[results_df['roc_auc'].idxmax()]
        best_auc_model = f"{best_auc_row['resampling_method']}_{best_auc_row['model_name']}"
        
        print(f"ğŸ† F1 Score ìµœê³  ëª¨ë¸: {best_f1_model} (F1: {best_f1_row['f1_score']:.4f})")
        print(f"ğŸ† ROC-AUC ìµœê³  ëª¨ë¸: {best_auc_model} (AUC: {best_auc_row['roc_auc']:.4f})")
        
        # ëŒ€ìƒ ëª¨ë¸ë“¤
        target_models = [best_f1_model, best_auc_model]
        if best_f1_model == best_auc_model:
            target_models = [best_f1_model]  # ê°™ì€ ëª¨ë¸ì´ë©´ í•˜ë‚˜ë§Œ
            print("ğŸ’¡ F1ê³¼ AUC ìµœê³  ëª¨ë¸ì´ ë™ì¼í•©ë‹ˆë‹¤.")
        
        shap_path = self.base_path / "results" / "07_shap_analysis"
        
        if not shap_path.exists():
            print("âš ï¸ SHAP ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê° ëª¨ë¸ì— ëŒ€í•´ beeplot ìƒì„±
        for i, model_name in enumerate(target_models):
            try:
                self.generate_single_beeplot(shap_path, model_name, i, len(target_models))
            except Exception as e:
                print(f"âŒ {model_name} beeplot ìƒì„± ì‹¤íŒ¨: {e}")
                
        print("âœ… ëª¨ë“  SHAP Beeplot ì €ì¥ ì™„ë£Œ")
    
    def generate_single_beeplot(self, shap_path, model_name, index, total_models):
        """ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•œ beeplot ìƒì„±"""
        
        # íŒŒì¼ ì°¾ê¸°
        shap_file = shap_path / f"{model_name}_shap_values.json"
        importance_file = shap_path / f"{model_name}_feature_importance.csv"
        
        if not shap_file.exists():
            print(f"âš ï¸ {model_name} SHAP ê°’ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {shap_file}")
            return
            
        if not importance_file.exists():
            print(f"âš ï¸ {model_name} íŠ¹ì„± ì¤‘ìš”ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {importance_file}")
            return
        
        print(f"\n=== {model_name} SHAP Beeplot ìƒì„± ===")
        print(f"SHAP íŒŒì¼: {shap_file.name}")
        print(f"íŠ¹ì„± ì¤‘ìš”ë„ íŒŒì¼: {importance_file.name}")
        
        # SHAP ê°’ ë¡œë“œ
        with open(shap_file, 'r') as f:
            shap_data = json.load(f)
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¡œë“œ (ìƒìœ„ 15ê°œ íŠ¹ì„±)
        importance_df = pd.read_csv(importance_file)
        top_features = importance_df.head(15)['feature'].tolist()
        
        print(f"ìƒìœ„ 15ê°œ íŠ¹ì„±: {top_features}")
        lactate_features = [f for f in top_features if 'lactate' in f.lower()]
        print(f"ì –ì‚° ê´€ë ¨ íŠ¹ì„±: {lactate_features}")

        # Figure ìƒì„±
        fig, axes = plt.subplots(1, 2, figsize=(20, 12))
        fig.suptitle(f'SHAP Beeplot Analysis - {model_name.replace("_", " ").title()}', 
                    fontsize=16, fontweight='bold')
        
        # 1. SHAP ê°’ ë¶„í¬ (Beeswarm-style plot)
        self.plot_shap_beeswarm(axes[0], shap_data, top_features, "SHAP Value Distribution")
        
        # 2. íŠ¹ì„±ë³„ SHAP ê°’ ìš”ì•½ í†µê³„
        self.plot_shap_summary_stats(axes[1], shap_data, top_features, "SHAP Value Statistics")
        
        plt.tight_layout()
        
        # ì €ì¥ (ì—¬ëŸ¬ ëª¨ë¸ì¸ ê²½ìš° ë²ˆí˜¸ ì¶”ê°€)
        if total_models > 1:
            beeplot_path = self.output_path / f"06_shap_beeplot_{index+1}_{model_name}.png"
        else:
            beeplot_path = self.output_path / "06_shap_beeplot.png"
            
        plt.savefig(beeplot_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"âœ… {model_name} Beeplot ì €ì¥: {beeplot_path.name}")
    
    def plot_shap_beeswarm(self, ax, shap_data, features, title):
        """SHAP ê°’ beeswarm plot"""
        try:
            # SHAP ê°’ê³¼ íŠ¹ì„± ê°’ ì¶”ì¶œ
            shap_values_raw = shap_data.get('shap_values', [])
            feature_values_raw = shap_data.get('feature_values', [])
            feature_names = shap_data.get('feature_names', [])
            
            print(f"SHAP values type: {type(shap_values_raw)}")
            print(f"Feature values type: {type(feature_values_raw)}")
            print(f"Feature names length: {len(feature_names)}")
            
            if not shap_values_raw or not feature_names:
                ax.text(0.5, 0.5, 'No SHAP values available', ha='center', va='center')
                ax.set_title(title)
                return
            
            # SHAP ê°’ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ë˜ ì°¨ì› í™•ì¸
            shap_values = np.array(shap_values_raw)
            feature_values = np.array(feature_values_raw) if feature_values_raw else None
            
            print(f"SHAP values shape: {shap_values.shape}")
            if feature_values is not None:
                print(f"Feature values shape: {feature_values.shape}")
            
            # 1ì°¨ì›ì¸ ê²½ìš° 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜
            if shap_values.ndim == 1:
                print("SHAP values is 1D, reshaping...")
                # íŠ¹ì„± ìˆ˜ë§Œí¼ ë¶„í• 
                n_features = len(feature_names)
                n_samples = len(shap_values) // n_features
                if len(shap_values) % n_features == 0:
                    shap_values = shap_values.reshape(n_samples, n_features)
                else:
                    ax.text(0.5, 0.5, 'SHAP values dimension mismatch', ha='center', va='center')
                    ax.set_title(title)
                    return
            
            # feature_valuesë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
            if feature_values is not None and feature_values.ndim == 1:
                print("Feature values is 1D, reshaping...")
                n_features = len(feature_names)
                n_samples = len(feature_values) // n_features
                if len(feature_values) % n_features == 0:
                    feature_values = feature_values.reshape(n_samples, n_features)
                else:
                    feature_values = None
            
            print(f"Final SHAP values shape: {shap_values.shape}")
            
            # ìƒìœ„ íŠ¹ì„±ë“¤ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
            feature_indices = []
            available_features = []
            lactate_found = []
            
            print(f"\n=== Feature Matching ë””ë²„ê¹… ===")
            print(f"ìš”ì²­ëœ features (ìƒìœ„ 15ê°œ): {features}")
            print(f"SHAP feature_names: {feature_names}")
            
            for feature in features:
                if feature in feature_names:
                    idx = feature_names.index(feature)
                    if idx < shap_values.shape[1]:  # ë²”ìœ„ í™•ì¸
                        feature_indices.append(idx)
                        available_features.append(feature)
                        if 'lactate' in feature.lower():
                            lactate_found.append(feature)
                        print(f"  âœ… {feature} -> index {idx}")
                    else:
                        print(f"  âŒ {feature} -> index {idx} (out of range)")
                else:
                    print(f"  âŒ {feature} -> not found in feature_names")
            
            print(f"ë°œê²¬ëœ ì –ì‚° íŠ¹ì„±ë“¤: {lactate_found}")
            
            if not feature_indices:
                ax.text(0.5, 0.5, 'No matching features found', ha='center', va='center')
                ax.set_title(title)
                return
            
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ features: {len(available_features)}")
            
            # ê° íŠ¹ì„±ë³„ë¡œ SHAP ê°’ í”Œë¡¯
            y_positions = []
            colors = plt.cm.RdYlBu_r  # ë¹¨ê°•-ë…¸ë‘-íŒŒë‘ ìƒ‰ìƒë§µ
            
            for i, feature_idx in enumerate(feature_indices):
                try:
                    feature_name = feature_names[feature_idx]
                    shap_vals = shap_values[:, feature_idx]
                    
                    # íŠ¹ì„± ê°’ ì²˜ë¦¬
                    if feature_values is not None and feature_idx < feature_values.shape[1]:
                        feat_vals = feature_values[:, feature_idx]
                    else:
                        # íŠ¹ì„± ê°’ì´ ì—†ìœ¼ë©´ SHAP ê°’ ìì²´ë¥¼ ì‚¬ìš©
                        feat_vals = shap_vals
                    
                    # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ì€ ì ì´ ìˆì„ ê²½ìš°)
                    if len(shap_vals) > 1000:
                        sample_idx = np.random.choice(len(shap_vals), 1000, replace=False)
                        shap_vals = shap_vals[sample_idx]
                        feat_vals = feat_vals[sample_idx]
                    
                    # íŠ¹ì„± ê°’ì— ë”°ë¥¸ ìƒ‰ìƒ ì •ê·œí™”
                    if np.std(feat_vals) > 0:
                        norm_feat_vals = (feat_vals - np.min(feat_vals)) / (np.max(feat_vals) - np.min(feat_vals))
                    else:
                        norm_feat_vals = np.zeros_like(feat_vals)
                    
                    # Y ìœ„ì¹˜ì— ì•½ê°„ì˜ jitter ì¶”ê°€
                    y_pos = np.full_like(shap_vals, i) + np.random.normal(0, 0.1, len(shap_vals))
                    y_positions.append(i)
                    
                    # ì‚°ì ë„ ê·¸ë¦¬ê¸°
                    scatter = ax.scatter(shap_vals, y_pos, c=norm_feat_vals, cmap=colors, 
                                       alpha=0.6, s=20, edgecolors='none')
                    
                except Exception as e:
                    print(f"Error processing feature {feature_idx}: {e}")
                    continue
            
            if not y_positions:
                ax.text(0.5, 0.5, 'No features could be plotted', ha='center', va='center')
                ax.set_title(title)
                return
            
            # ì¶• ì„¤ì •
            ax.set_yticks(y_positions)
            ax.set_yticklabels([available_features[i][:30] + '...' if len(available_features[i]) > 30 else available_features[i] 
                               for i in range(len(y_positions))], fontsize=10)
            ax.set_xlabel('SHAP Value (Impact on Model Output)')
            ax.set_title(title, fontweight='bold')
            ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)
            ax.grid(True, alpha=0.3)
            
            # ì»¬ëŸ¬ë°” ì¶”ê°€ (scatterê°€ ì •ì˜ëœ ê²½ìš°ì—ë§Œ)
            try:
                cbar = plt.colorbar(scatter, ax=ax, fraction=0.02, pad=0.01)
                cbar.set_label('Feature Value\n(Low â†’ High)', rotation=270, labelpad=20)
            except:
                pass
            
        except Exception as e:
            ax.text(0.5, 0.5, f'Error creating beeswarm plot: {str(e)}', ha='center', va='center')
            ax.set_title(title)
            print(f"Beeswarm plot error: {e}")
            import traceback
            traceback.print_exc()
    
    def plot_shap_summary_stats(self, ax, shap_data, features, title):
        """SHAP ê°’ ìš”ì•½ í†µê³„"""
        try:
            shap_values_raw = shap_data.get('shap_values', [])
            feature_names = shap_data.get('feature_names', [])
            
            if not shap_values_raw or not feature_names:
                ax.text(0.5, 0.5, 'No SHAP values available', ha='center', va='center')
                ax.set_title(title)
                return
            
            # SHAP ê°’ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ë˜ ì°¨ì› í™•ì¸
            shap_values = np.array(shap_values_raw)
            
            # 1ì°¨ì›ì¸ ê²½ìš° 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜
            if shap_values.ndim == 1:
                n_features = len(feature_names)
                n_samples = len(shap_values) // n_features
                if len(shap_values) % n_features == 0:
                    shap_values = shap_values.reshape(n_samples, n_features)
                else:
                    ax.text(0.5, 0.5, 'SHAP values dimension mismatch', ha='center', va='center')
                    ax.set_title(title)
                    return
            
            # íŠ¹ì„±ë³„ í†µê³„ ê³„ì‚°
            stats_data = []
            for feature in features:
                if feature in feature_names:
                    feature_idx = feature_names.index(feature)
                    if feature_idx < shap_values.shape[1]:  # ë²”ìœ„ í™•ì¸
                        try:
                            shap_vals = shap_values[:, feature_idx]
                            
                            stats = {
                                'feature': feature[:25] + '...' if len(feature) > 25 else feature,
                                'mean_abs': np.mean(np.abs(shap_vals)),
                                'std': np.std(shap_vals),
                                'min': np.min(shap_vals),
                                'max': np.max(shap_vals)
                            }
                            stats_data.append(stats)
                        except Exception as e:
                            print(f"Error processing feature {feature}: {e}")
                            continue
            
            if not stats_data:
                ax.text(0.5, 0.5, 'No statistics available', ha='center', va='center')
                ax.set_title(title)
                return
            
            # í‰ê·  ì ˆëŒ“ê°’ìœ¼ë¡œ ì •ë ¬
            stats_data.sort(key=lambda x: x['mean_abs'], reverse=True)
            
            # ë°•ìŠ¤í”Œë¡¯ ìŠ¤íƒ€ì¼ë¡œ ì‹œê°í™”
            y_positions = range(len(stats_data))
            feature_names_short = [item['feature'] for item in stats_data]
            
            # í‰ê·  ì ˆëŒ“ê°’ ë§‰ëŒ€ê·¸ë˜í”„
            mean_abs_vals = [item['mean_abs'] for item in stats_data]
            bars = ax.barh(y_positions, mean_abs_vals, color=self.colors['primary'], alpha=0.7)
            
            # ìµœì†Ÿê°’, ìµœëŒ“ê°’ ë²”ìœ„ í‘œì‹œ
            for i, item in enumerate(stats_data):
                ax.plot([item['min'], item['max']], [i, i], 'k-', alpha=0.5, linewidth=2)
                ax.plot([item['min']], [i], 'k|', markersize=8, alpha=0.7)
                ax.plot([item['max']], [i], 'k|', markersize=8, alpha=0.7)
            
            ax.set_yticks(y_positions)
            ax.set_yticklabels(feature_names_short, fontsize=10)
            ax.set_xlabel('Mean |SHAP Value|')
            ax.set_title(title, fontweight='bold')
            ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)
            ax.grid(True, alpha=0.3, axis='x')
            
            # ê°’ í‘œì‹œ
            for i, (bar, val) in enumerate(zip(bars, mean_abs_vals)):
                ax.text(val + val*0.01, bar.get_y() + bar.get_height()/2, 
                       f'{val:.3f}', ha='left', va='center', fontsize=8)
            
            ax.invert_yaxis()
            
        except Exception as e:
            ax.text(0.5, 0.5, f'Error creating summary stats: {str(e)}', ha='center', va='center')
            ax.set_title(title)
            print(f"Summary stats error: {e}")
            import traceback
            traceback.print_exc()
    
    def generate_all_figures(self):
        """ëª¨ë“  ì‹œê°í™” ìƒì„±"""
        print("ğŸ¨ MIMIC-IV 48ì‹œê°„ ì‚¬ë§ë¥  ì˜ˆì¸¡ - ì „ì²´ ì‹œê°í™” ìƒì„± ì‹œì‘")
        print("=" * 70)
        
        # ì¶œë ¥ í´ë” ì •ë³´
        print(f"ğŸ“ ì‹œê°í™” ì €ì¥ ìœ„ì¹˜: {self.output_path}")
        
        # ê° ì‹œê°í™” ìƒì„±
        self.create_data_distribution_plots()
        self.create_missing_data_heatmap()
        self.create_resampling_comparison()
        self.create_model_performance_plots()
        self.create_final_dashboard()
        self.generate_shap_visualizations()
        
        print("\n" + "=" * 70)
        print("ğŸ‰ ëª¨ë“  ì‹œê°í™” ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼:")
        
        # ìƒì„±ëœ íŒŒì¼ ëª©ë¡ (ë™ì ìœ¼ë¡œ í™•ì¸)
        expected_files = [
            "01_data_distribution.png",
            "02_missing_data_analysis.png",
            "02_missing_data_impact.png", 
            "03_resampling_comparison.png",
            "03_data_pipeline.png",
            "04_model_performance.png",
            "05_final_dashboard.png",
            "05_shap_analysis.png"
        ]
        
        # SHAP beeplot íŒŒì¼ë“¤ ë™ì  ì¶”ê°€
        beeplot_files = list(self.output_path.glob("06_shap_beeplot*.png"))
        for beeplot_file in beeplot_files:
            expected_files.append(beeplot_file.name)
        
        for i, filename in enumerate(expected_files, 1):
            file_path = self.output_path / filename
            if file_path.exists():
                print(f"  âœ… {i}. {filename}")
            else:
                print(f"  âŒ {i}. {filename} (ìƒì„±ë˜ì§€ ì•ŠìŒ)")
        
        print(f"\nğŸ“Š ì´ {len(expected_files)}ê°œ ì‹œê°í™” íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("=" * 70)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    generator = FigureGenerator()
    generator.generate_all_figures()

if __name__ == "__main__":
    main()
