#!/usr/bin/env python3
"""
ë¦¬ìƒ˜í”Œë§
- í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ì ìš©
- SMOTE (Synthetic Minority Oversampling Technique)
- Downsampling (ë‹¤ìš´ìƒ˜í”Œë§)
- Train ì„¸íŠ¸ì—ë§Œ ì ìš©, Validation/TestëŠ” ì›ë³¸ ë¶„í¬ ìœ ì§€
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.utils import resample
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

class DataResampler:
    def __init__(self, input_path, output_path):
        self.input_path = Path(input_path)
        self.output_path = Path(output_path)
        self.target_column = 'mortality_48h'
        
    def create_output_directories(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        self.original_path = self.output_path / "original"
        self.smote_path = self.output_path / "smote"
        self.downsampling_path = self.output_path / "downsampling"
        
        self.original_path.mkdir(parents=True, exist_ok=True)
        self.smote_path.mkdir(parents=True, exist_ok=True)
        self.downsampling_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ Original ì¶œë ¥: {self.original_path}")
        print(f"ğŸ“ SMOTE ì¶œë ¥: {self.smote_path}")
        print(f"ğŸ“ Downsampling ì¶œë ¥: {self.downsampling_path}")
    
    def load_split_data(self):
        """ë¶„í• ëœ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ ë¶„í• ëœ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        train_df = pd.read_csv(self.input_path / "mimic_mortality_train.csv")
        val_df = pd.read_csv(self.input_path / "mimic_mortality_validation.csv")
        test_df = pd.read_csv(self.input_path / "mimic_mortality_test.csv")
        
        print(f"âœ… Train: {train_df.shape}")
        print(f"âœ… Validation: {val_df.shape}")
        print(f"âœ… Test: {test_df.shape}")
        
        return train_df, val_df, test_df
    
    def analyze_class_distribution(self, df, title="í´ë˜ìŠ¤ ë¶„í¬"):
        """í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„"""
        print(f"\nğŸ“Š {title}")
        
        if self.target_column in df.columns:
            dist = df[self.target_column].value_counts().sort_index()
            total = len(df)
            
            print(f"   - ì´ ìƒ˜í”Œ: {total:,}ê°œ")
            print(f"   - ìƒì¡´ (0): {dist[0]:,}ê°œ ({dist[0]/total:.1%})")
            print(f"   - ì‚¬ë§ (1): {dist[1]:,}ê°œ ({dist[1]/total:.1%})")
            print(f"   - ë¶ˆê· í˜• ë¹„ìœ¨: {dist[0]/dist[1]:.1f}:1")
            
            return dist
        
        return None
    
    def apply_downsampling(self, train_df):
        """ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©"""
        print("\nâ¬‡ï¸ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš© ì¤‘...")
        
        # í´ë˜ìŠ¤ë³„ ë¶„ë¦¬
        majority_class = train_df[train_df[self.target_column] == 0]  # ìƒì¡´
        minority_class = train_df[train_df[self.target_column] == 1]  # ì‚¬ë§
        
        print(f"   ì›ë³¸ - ìƒì¡´: {len(majority_class):,}ê°œ, ì‚¬ë§: {len(minority_class):,}ê°œ")
        
        # ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ì†Œìˆ˜ í´ë˜ìŠ¤ ìˆ˜ì— ë§ì¶° ë‹¤ìš´ìƒ˜í”Œë§
        majority_downsampled = resample(
            majority_class,
            replace=False,
            n_samples=len(minority_class),
            random_state=42
        )
        
        # ê· í˜• ì¡íŒ ë°ì´í„°ì…‹ ìƒì„±
        balanced_df = pd.concat([majority_downsampled, minority_class])
        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"   âœ… ë‹¤ìš´ìƒ˜í”Œë§ í›„ - ì´: {len(balanced_df):,}ê°œ")
        self.analyze_class_distribution(balanced_df, "ë‹¤ìš´ìƒ˜í”Œë§ í›„ ë¶„í¬")
        
        return balanced_df
    
    def apply_smote(self, train_df):
        """SMOTE ì ìš©"""
        print("\nâ¬†ï¸ SMOTE ì ìš© ì¤‘...")
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        X = train_df.drop(columns=[self.target_column])
        y = train_df[self.target_column]
        
        print(f"   ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬: {Counter(y)}")
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ ì¸ì½”ë”©
        label_encoders = {}
        X_encoded = X.copy()
        
        categorical_columns = X.select_dtypes(include=['object']).columns
        if len(categorical_columns) > 0:
            print(f"   ë¬¸ìì—´ ì»¬ëŸ¼ ì¸ì½”ë”©: {list(categorical_columns)}")
            
            for col in categorical_columns:
                le = LabelEncoder()
                X_encoded[col] = le.fit_transform(X[col].astype(str))
                label_encoders[col] = le
        
        # ê²°ì¸¡ì¹˜ ì œê±° (SMOTEëŠ” ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒ)
        if X_encoded.isnull().any().any():
            print("   âš ï¸ SMOTE ì ìš© ì „ ê²°ì¸¡ì¹˜ ì œê±°")
            missing_mask = X_encoded.isnull().any(axis=1)
            X_encoded = X_encoded[~missing_mask]
            y = y[~missing_mask]
            print(f"   ê²°ì¸¡ì¹˜ ì œê±° í›„: {len(X_encoded):,}ê°œ ìƒ˜í”Œ")
        
        # SMOTE ì ìš©
        try:
            smote = SMOTE(
                random_state=42,
                k_neighbors=min(5, (y == 1).sum() - 1),  # ì†Œìˆ˜ í´ë˜ìŠ¤ í¬ê¸°ì— ë§ì¶° ì¡°ì •
                sampling_strategy='auto'
            )
            
            X_resampled, y_resampled = smote.fit_resample(X_encoded, y)
            print(f"   SMOTE í›„ í´ë˜ìŠ¤ ë¶„í¬: {Counter(y_resampled)}")
            
        except Exception as e:
            print(f"   âŒ SMOTE ì ìš© ì‹¤íŒ¨: {e}")
            print("   ì›ë³¸ ë°ì´í„° ë°˜í™˜")
            return train_df
        
        # ë°ì´í„°í”„ë ˆì„ ì¬êµ¬ì„±
        balanced_df = pd.DataFrame(X_resampled, columns=X_encoded.columns)
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ ë””ì½”ë”©
        for col, le in label_encoders.items():
            # ë°˜ì˜¬ë¦¼ í›„ í´ë¦¬í•‘ìœ¼ë¡œ ìœ íš¨í•œ ë²”ìœ„ ë³´ì¥
            balanced_df[col] = np.round(balanced_df[col]).astype(int)
            balanced_df[col] = np.clip(balanced_df[col], 0, len(le.classes_) - 1)
            balanced_df[col] = le.inverse_transform(balanced_df[col])
        
        # íƒ€ê²Ÿ ì¶”ê°€
        balanced_df[self.target_column] = y_resampled
        
        print(f"   âœ… SMOTE í›„ - ì´: {len(balanced_df):,}ê°œ")
        self.analyze_class_distribution(balanced_df, "SMOTE í›„ ë¶„í¬")
        
        return balanced_df
    
    def save_resampled_data(self, method, train_resampled, val_df, test_df):
        """ë¦¬ìƒ˜í”Œë§ëœ ë°ì´í„° ì €ì¥"""
        print(f"\nğŸ’¾ {method} ë°ì´í„° ì €ì¥ ì¤‘...")
        
        # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
        if method == "Original":
            output_dir = self.original_path
        elif method == "SMOTE":
            output_dir = self.smote_path
        else:  # Downsampling
            output_dir = self.downsampling_path
        
        # íŒŒì¼ ì €ì¥
        datasets = {
            'train': train_resampled,
            'validation': val_df,
            'test': test_df
        }
        
        saved_files = {}
        
        for name, df in datasets.items():
            filename = f"mimic_mortality_{name}.csv"
            filepath = output_dir / filename
            df.to_csv(filepath, index=False)
            saved_files[name] = filepath
            print(f"   âœ… {name}: {len(df):,}ê°œ â†’ {filepath}")
        
        return saved_files
    
    def save_resampling_summary(self, train_original, train_smote, train_down, val_df, test_df):
        """ë¦¬ìƒ˜í”Œë§ ìš”ì•½ ì €ì¥"""
        summary_file = self.output_path / "resampling_summary.txt"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("ë¦¬ìƒ˜í”Œë§ ìš”ì•½\n")
            f.write("=" * 50 + "\n")
            f.write(f"ì²˜ë¦¬ ì¼ì‹œ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("ë¦¬ìƒ˜í”Œë§ ë°©ë²•:\n")
            f.write("1. Original (ì›ë³¸)\n")
            f.write("   - ë¦¬ìƒ˜í”Œë§ ì—†ì´ ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©\n")
            f.write("2. SMOTE (Synthetic Minority Oversampling Technique)\n")
            f.write("   - ì†Œìˆ˜ í´ë˜ìŠ¤ í•©ì„± ë°ì´í„° ìƒì„±ìœ¼ë¡œ ê· í˜• ë§ì¶¤\n")
            f.write("3. Downsampling (ë‹¤ìš´ìƒ˜í”Œë§)\n")
            f.write("   - ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ì†Œìˆ˜ í´ë˜ìŠ¤ ìˆ˜ì¤€ìœ¼ë¡œ ì¶•ì†Œ\n\n")
            
            f.write("ì ìš© ì›ì¹™:\n")
            f.write("- Train ì„¸íŠ¸ì—ë§Œ ë¦¬ìƒ˜í”Œë§ ì ìš©\n")
            f.write("- Validation/Test ì„¸íŠ¸ëŠ” ì›ë³¸ ë¶„í¬ ìœ ì§€ (ëª¨ë“  ë°©ë²•ì—ì„œ ë™ì¼)\n")
            f.write("- ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•œ ì˜¬ë°”ë¥¸ ë¶„í•  í›„ ì²˜ë¦¬\n\n")
            
            # ë°ì´í„° í¬ê¸° ë¹„êµ
            f.write("ë°ì´í„° í¬ê¸° ë¹„êµ:\n")
            f.write(f"- Original Train: {len(train_original):,}ê°œ\n")
            f.write(f"- SMOTE Train: {len(train_smote):,}ê°œ\n")
            f.write(f"- Downsampling Train: {len(train_down):,}ê°œ\n")
            f.write(f"- Validation: {len(val_df):,}ê°œ (ëª¨ë“  ë°©ë²•ì—ì„œ ë™ì¼)\n")
            f.write(f"- Test: {len(test_df):,}ê°œ (ëª¨ë“  ë°©ë²•ì—ì„œ ë™ì¼)\n\n")
            
            # í´ë˜ìŠ¤ ë¶„í¬ ë¹„êµ
            f.write("48ì‹œê°„ ì‚¬ë§ë¥  ë¹„êµ:\n")
            datasets = [
                ("Original Train", train_original),
                ("SMOTE Train", train_smote), 
                ("Downsampling Train", train_down),
                ("Validation", val_df),
                ("Test", test_df)
            ]
            
            for name, df in datasets:
                if self.target_column in df.columns:
                    mortality_rate = df[self.target_column].mean()
                    mortality_count = df[self.target_column].sum()
                    f.write(f"- {name}: {mortality_rate:.1%} ({mortality_count:,}ëª…)\n")
            
            f.write(f"\nì €ì¥ ìœ„ì¹˜:\n")
            f.write(f"- Original: {self.original_path}\n")
            f.write(f"- SMOTE: {self.smote_path}\n")
            f.write(f"- Downsampling: {self.downsampling_path}\n")
            
            f.write(f"\në‹¤ìŒ ë‹¨ê³„: ëª¨ë¸ë§ ë° í‰ê°€ (05_modeling_evaluation.py)\n")
        
        print(f"âœ… ë¦¬ìƒ˜í”Œë§ ìš”ì•½: {summary_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("âš–ï¸ ë¦¬ìƒ˜í”Œë§ ì‹œì‘")
    print("=" * 60)
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
    project_root = Path(__file__).parent.parent
    
    # ê²½ë¡œ ì„¤ì • - ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
    input_path = project_root / "dataset" / "2_split"
    output_path = project_root / "dataset" / "3_resampled"
    
    # ë¦¬ìƒ˜í”ŒëŸ¬ ì´ˆê¸°í™”
    resampler = DataResampler(input_path, output_path)
    resampler.create_output_directories()
    
    # 1. ë¶„í• ëœ ë°ì´í„° ë¡œë“œ
    train_df, val_df, test_df = resampler.load_split_data()
    
    # 2. ì›ë³¸ train ë¶„í¬ ë¶„ì„
    resampler.analyze_class_distribution(train_df, "ì›ë³¸ Train ë¶„í¬")
    
    # 3. Original ë°ì´í„° ì €ì¥ (ë¦¬ìƒ˜í”Œë§ ì—†ìŒ)
    original_files = resampler.save_resampled_data("Original", train_df.copy(), val_df, test_df)
    
    # 4. SMOTE ì ìš©
    train_smote = resampler.apply_smote(train_df.copy())
    
    # 5. ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©  
    train_downsampled = resampler.apply_downsampling(train_df.copy())
    
    # 6. ë¦¬ìƒ˜í”Œë§ëœ ë°ì´í„° ì €ì¥
    smote_files = resampler.save_resampled_data("SMOTE", train_smote, val_df, test_df)
    down_files = resampler.save_resampled_data("Downsampling", train_downsampled, val_df, test_df)
    
    # 7. ìš”ì•½ ì •ë³´ ì €ì¥
    resampler.save_resampling_summary(train_df, train_smote, train_downsampled, val_df, test_df)
    
    print("\n" + "=" * 60)
    print("âœ… ë¦¬ìƒ˜í”Œë§ ì™„ë£Œ!")
    print(f"ğŸ“Š ê²°ê³¼:")
    print(f"   - Original Train: {len(train_df):,}ê°œ (ë¦¬ìƒ˜í”Œë§ ì—†ìŒ)")
    print(f"   - SMOTE Train: {len(train_smote):,}ê°œ")
    print(f"   - Downsampling Train: {len(train_downsampled):,}ê°œ")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜:")
    print(f"   - Original: {output_path}/original/")
    print(f"   - SMOTE: {output_path}/smote/")
    print(f"   - Downsampling: {output_path}/downsampling/")
    print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: 05_modeling_evaluation.py ì‹¤í–‰")
    print("=" * 60)

if __name__ == "__main__":
    main()
