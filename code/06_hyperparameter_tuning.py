#!/usr/bin/env python3
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- Optunaë¥¼ ì‚¬ìš©í•œ ë² ì´ì§€ì•ˆ ìµœì í™”
- ìµœì¢… í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
"""

import pandas as pd
import numpy as np
from pathlib import Path
import joblib
import optuna
import warnings
warnings.filterwarnings('ignore')

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import cross_val_score
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix
)
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

class HyperparameterTuner:
    def __init__(self, modeling_results_path, resampling_path, output_path):
        self.modeling_results_path = Path(modeling_results_path)
        self.resampling_path = Path(resampling_path)
        self.output_path = Path(output_path)
        self.output_path.mkdir(exist_ok=True)
        
        self.target_column = 'mortality_48h'
        self.tuned_models = {}
        self.final_results = []
        
    def load_modeling_results(self):
        """ëª¨ë¸ë§ ê²°ê³¼ ë¡œë“œí•˜ì—¬ ëª¨ë“  ëª¨ë¸ ìœ í˜• í™•ì¸"""
        print("ğŸ“Š ëª¨ë¸ë§ ê²°ê³¼ ë¡œë“œ ì¤‘...")
        
        results_file = self.modeling_results_path / "modeling_results.csv"
        results_df = pd.read_csv(results_file)
        
        # ëª¨ë“  ê³ ìœ  ëª¨ë¸ ìœ í˜• ì¶”ì¶œ
        unique_models = results_df['model_name'].unique()
        
        print("ğŸ¯ íŠœë‹ ëŒ€ìƒ ëª¨ë¸ ìœ í˜•:")
        for idx, model_name in enumerate(unique_models):
            best_for_model = results_df[results_df['model_name'] == model_name].iloc[0]
            print(f"  {idx+1}. {model_name} (ìµœê³  ROC-AUC: {best_for_model['roc_auc']:.4f})")
        
        return results_df
    
    def load_resampling_data(self, resampling_method):
        """íŠ¹ì • ë¦¬ìƒ˜í”Œë§ ë°©ë²•ì˜ ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“‚ {resampling_method.upper()} ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        data_path = self.resampling_path / resampling_method
        
        train_df = pd.read_csv(data_path / "mimic_mortality_train.csv")
        val_df = pd.read_csv(data_path / "mimic_mortality_validation.csv")
        test_df = pd.read_csv(data_path / "mimic_mortality_test.csv")
        
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: Train({len(train_df)}), Val({len(val_df)}), Test({len(test_df)})")
        return train_df, val_df, test_df
    
    def preprocess_data(self, train_df, val_df, test_df):
        """ë°ì´í„° ì „ì²˜ë¦¬ (06ë²ˆê³¼ ë™ì¼)"""
        print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬...")
        
        # ID ì»¬ëŸ¼ ì œê±°
        id_columns = ['subject_id', 'hadm_id', 'stay_id']
        existing_id_cols = [col for col in id_columns if col in train_df.columns]
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        X_train = train_df.drop(columns=existing_id_cols + [self.target_column])
        y_train = train_df[self.target_column]
        X_val = val_df.drop(columns=existing_id_cols + [self.target_column])
        y_val = val_df[self.target_column]
        X_test = test_df.drop(columns=existing_id_cols + [self.target_column])
        y_test = test_df[self.target_column]
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ ì¸ì½”ë”© (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
        from sklearn.preprocessing import LabelEncoder
        
        categorical_columns = X_train.select_dtypes(include=['object']).columns
        label_encoders = {}
        
        for col in categorical_columns:
            le = LabelEncoder()
            # Train ë°ì´í„°ë¡œë§Œ fití•˜ì—¬ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€
            le.fit(X_train[col].astype(str))
            label_encoders[col] = le
            
            # ê° ì„¸íŠ¸ ë³€í™˜
            X_train[col] = le.transform(X_train[col].astype(str))
            
            # Validationê³¼ Testì—ì„œ ìƒˆë¡œìš´ ê°’ ì²˜ë¦¬
            for val in X_val[col].astype(str):
                if val not in le.classes_:
                    # ìƒˆë¡œìš´ ê°’ì„ ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´
                    most_frequent = X_train[col].mode()[0] if not X_train[col].empty else 0
                    X_val.loc[X_val[col].astype(str) == val, col] = most_frequent
            X_val[col] = le.transform(X_val[col].astype(str))
            
            for val in X_test[col].astype(str):
                if val not in le.classes_:
                    # ìƒˆë¡œìš´ ê°’ì„ ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´
                    most_frequent = X_train[col].mode()[0] if not X_train[col].empty else 0
                    X_test.loc[X_test[col].astype(str) == val, col] = most_frequent
            X_test[col] = le.transform(X_test[col].astype(str))
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        for col in X_train.columns:
            if X_train[col].isnull().any():
                mean_val = X_train[col].mean()
                X_train[col].fillna(mean_val, inplace=True)
                X_val[col].fillna(mean_val, inplace=True)
                X_test[col].fillna(mean_val, inplace=True)
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def tune_xgboost(self, X_train, X_val, y_train, y_val, n_trials=50):
        """XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        print("ğŸ¯ XGBoost íŠœë‹ ì¤‘...")
        
        def objective(trial):
            params = {
                'objective': 'binary:logistic',
                'eval_metric': 'auc',
                'verbosity': 0,
                'random_state': 42,
                
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 8),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            }
            
            model = xgb.XGBClassifier(**params)
            
            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
            
            y_pred_proba = model.predict_proba(X_val)[:, 1]
            return roc_auc_score(y_val, y_pred_proba)
        
        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        # ìµœì  ëª¨ë¸ í•™ìŠµ
        best_params = study.best_params
        best_params.update({'objective': 'binary:logistic', 'eval_metric': 'auc', 'verbosity': 0, 'random_state': 42})
        
        best_model = xgb.XGBClassifier(**best_params)
        best_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
        
        print(f"   ìµœì  ROC-AUC: {study.best_value:.4f}")
        return best_model, best_params
    
    def tune_lightgbm(self, X_train, X_val, y_train, y_val, n_trials=50):
        """LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        print("ğŸ¯ LightGBM íŠœë‹ ì¤‘...")
        
        def objective(trial):
            params = {
                'objective': 'binary',
                'metric': 'auc',
                'verbosity': -1,
                'random_state': 42,
                
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 8),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
                'num_leaves': trial.suggest_int('num_leaves', 10, 100),
            }
            
            model = lgb.LGBMClassifier(**params)
            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])
            
            y_pred_proba = model.predict_proba(X_val)[:, 1]
            return roc_auc_score(y_val, y_pred_proba)
        
        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        # ìµœì  ëª¨ë¸ í•™ìŠµ
        best_params = study.best_params
        best_params.update({'objective': 'binary', 'metric': 'auc', 'verbosity': -1, 'random_state': 42})
        
        best_model = lgb.LGBMClassifier(**best_params)
        best_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])
        
        print(f"   ìµœì  ROC-AUC: {study.best_value:.4f}")
        return best_model, best_params
    
    def tune_random_forest(self, X_train, X_val, y_train, y_val, n_trials=30):
        """Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        print("ğŸ¯ Random Forest íŠœë‹ ì¤‘...")
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 200),
                'max_depth': trial.suggest_int('max_depth', 5, 20),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
                'class_weight': 'balanced',
                'random_state': 42,
                'n_jobs': -1
            }
            
            model = RandomForestClassifier(**params)
            model.fit(X_train, y_train)
            
            y_pred_proba = model.predict_proba(X_val)[:, 1]
            return roc_auc_score(y_val, y_pred_proba)
        
        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        # ìµœì  ëª¨ë¸ í•™ìŠµ
        best_params = study.best_params
        best_params.update({'class_weight': 'balanced', 'random_state': 42, 'n_jobs': -1})
        
        best_model = RandomForestClassifier(**best_params)
        best_model.fit(X_train, y_train)
        
        print(f"   ìµœì  ROC-AUC: {study.best_value:.4f}")
        return best_model, best_params
    
    def tune_extra_trees(self, X_train, X_val, y_train, y_val, n_trials=30):
        """Extra Trees í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        print("ğŸ¯ Extra Trees íŠœë‹ ì¤‘...")
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 200),
                'max_depth': trial.suggest_int('max_depth', 5, 20),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
                'class_weight': 'balanced',
                'random_state': 42,
                'n_jobs': -1
            }
            
            model = ExtraTreesClassifier(**params)
            model.fit(X_train, y_train)
            
            y_pred_proba = model.predict_proba(X_val)[:, 1]
            return roc_auc_score(y_val, y_pred_proba)
        
        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        # ìµœì  ëª¨ë¸ í•™ìŠµ
        best_params = study.best_params
        best_params.update({'class_weight': 'balanced', 'random_state': 42, 'n_jobs': -1})
        
        best_model = ExtraTreesClassifier(**best_params)
        best_model.fit(X_train, y_train)
        
        print(f"   ìµœì  ROC-AUC: {study.best_value:.4f}")
        return best_model, best_params
    
    def tune_svc(self, X_train, X_val, y_train, y_val, n_trials=30):
        """SVC í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        print("ğŸ¯ SVC íŠœë‹ ì¤‘...")
        
        def objective(trial):
            params = {
                'C': trial.suggest_float('C', 0.1, 10.0, log=True),
                'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),
                'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear']),
                'class_weight': 'balanced',
                'probability': True,
                'random_state': 42
            }
            
            model = SVC(**params)
            model.fit(X_train, y_train)
            
            y_pred_proba = model.predict_proba(X_val)[:, 1]
            return roc_auc_score(y_val, y_pred_proba)
        
        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        # ìµœì  ëª¨ë¸ í•™ìŠµ
        best_params = study.best_params
        best_params.update({'class_weight': 'balanced', 'probability': True, 'random_state': 42})
        
        best_model = SVC(**best_params)
        best_model.fit(X_train, y_train)
        
        print(f"   ìµœì  ROC-AUC: {study.best_value:.4f}")
        return best_model, best_params
    
    def tune_logistic_regression(self, X_train, X_val, y_train, y_val, n_trials=30):
        """Logistic Regression í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        print("ğŸ¯ Logistic Regression íŠœë‹ ì¤‘...")
        
        def objective(trial):
            params = {
                'C': trial.suggest_float('C', 0.1, 10.0, log=True),
                'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),
                'solver': 'liblinear',
                'class_weight': 'balanced',
                'random_state': 42,
                'max_iter': 1000
            }
            
            model = LogisticRegression(**params)
            model.fit(X_train, y_train)
            
            y_pred_proba = model.predict_proba(X_val)[:, 1]
            return roc_auc_score(y_val, y_pred_proba)
        
        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        # ìµœì  ëª¨ë¸ í•™ìŠµ
        best_params = study.best_params
        best_params.update({'class_weight': 'balanced', 'random_state': 42, 'max_iter': 1000, 'solver': 'liblinear'})
        
        best_model = LogisticRegression(**best_params)
        best_model.fit(X_train, y_train)
        
        print(f"   ìµœì  ROC-AUC: {study.best_value:.4f}")
        return best_model, best_params
    
    def evaluate_final_model(self, model, model_name, X_test, y_test):
        """ìµœì¢… í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€"""
        print(f"ğŸ“Š {model_name} ìµœì¢… í‰ê°€...")
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # ì§€í‘œ ê³„ì‚°
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        
        metrics = {
            'model_name': model_name,
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1_score': f1_score(y_test, y_pred, zero_division=0),
            'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,
            'roc_auc': roc_auc_score(y_test, y_pred_proba),
            'pr_auc': average_precision_score(y_test, y_pred_proba),
            'confusion_matrix': {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}
        }
        
        print(f"   ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"   F1-Score: {metrics['f1_score']:.4f}")
        print(f"   Precision: {metrics['precision']:.4f}")
        print(f"   Recall: {metrics['recall']:.4f}")
        
        return metrics
    
    def run_tuning(self):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰"""
        print("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
        print("=" * 60)
        
        # ëª¨ë¸ë§ ê²°ê³¼ ë¡œë“œ
        all_results = self.load_modeling_results()
        
        # ëª¨ë“  ê³ ìœ  ëª¨ë¸ ìœ í˜• ì¶”ì¶œ
        unique_models = all_results['model_name'].unique()
        
        # ëª¨ë“  ë¦¬ìƒ˜í”Œë§ ë°©ë²• ì •ì˜
        all_resampling_methods = ['original', 'smote', 'downsampling']
        
        # ê° ëª¨ë¸ë³„ ì²˜ë¦¬
        for model_name in unique_models:
            print(f"\nğŸ”§ {model_name} íŠœë‹ ì‹œì‘...")
            
            # XGBoostì™€ LightGBMì€ ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´ íŠœë‹
            if model_name in ['XGBoost', 'LightGBM']:
                print(f"   ğŸ¯ {model_name}: ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´ íŠœë‹ ìˆ˜í–‰")
                
                for resampling_method in all_resampling_methods:
                    print(f"\n   ğŸ“‚ {resampling_method.upper()} ë°ì´í„°ì…‹ìœ¼ë¡œ {model_name} íŠœë‹...")
                    
                    try:
                        # í•´ë‹¹ ë¦¬ìƒ˜í”Œë§ ë°ì´í„° ë¡œë“œ
                        train_df, val_df, test_df = self.load_resampling_data(resampling_method)
                        
                        # ë°ì´í„° ì „ì²˜ë¦¬
                        X_train, X_val, X_test, y_train, y_val, y_test = self.preprocess_data(train_df, val_df, test_df)
                        
                        # ëª¨ë¸ íŠœë‹
                        if model_name == 'XGBoost':
                            tuned_model, best_params = self.tune_xgboost(X_train, X_val, y_train, y_val)
                        elif model_name == 'LightGBM':
                            tuned_model, best_params = self.tune_lightgbm(X_train, X_val, y_train, y_val)
                        
                        # ëª¨ë¸ ì €ì¥
                        model_filename = f"tuned_{model_name.lower()}_{resampling_method}.pkl"
                        model_path = self.output_path / model_filename
                        joblib.dump(tuned_model, model_path)
                        
                        # ìµœì¢… í‰ê°€
                        final_metrics = self.evaluate_final_model(tuned_model, f"{model_name}_{resampling_method}", X_test, y_test)
                        final_metrics['best_params'] = best_params
                        final_metrics['model_path'] = str(model_path)
                        final_metrics['resampling_method'] = resampling_method
                        final_metrics['base_model'] = model_name
                        
                        self.final_results.append(final_metrics)
                        print(f"      âœ… {model_name}_{resampling_method} íŠœë‹ ì™„ë£Œ!")
                        
                    except Exception as e:
                        print(f"      âŒ {model_name}_{resampling_method} íŠœë‹ ì‹¤íŒ¨: {str(e)}")
                        continue
            
            # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ íŠœë‹í•˜ì§€ ì•ŠìŒ
            else:
                print(f"   âš ï¸ {model_name}: íŠœë‹ ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        print("\nğŸ’¾ íŠœë‹ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        if not self.final_results:
            print("âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê²°ê³¼ DataFrame ìƒì„± (best_params ì œì™¸)
        results_data = []
        for result in self.final_results:
            result_copy = result.copy()
            del result_copy['best_params']  # íŒŒë¼ë¯¸í„°ëŠ” ë³„ë„ ì €ì¥
            del result_copy['confusion_matrix']  # í˜¼ë™í–‰ë ¬ ì œì™¸
            results_data.append(result_copy)
        
        results_df = pd.DataFrame(results_data)
        results_df = results_df.sort_values('roc_auc', ascending=False)
        
        # CSV ì €ì¥
        results_csv = self.output_path / "tuning_results.csv"
        results_df.to_csv(results_csv, index=False)
        print(f"âœ… íŠœë‹ ê²°ê³¼: {results_csv}")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸
        self.create_final_report(results_df)
    
    def create_final_report(self, results_df):
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        summary_file = self.output_path / "final_report.txt"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("MIMIC-IV 48ì‹œê°„ ì‚¬ë§ë¥  ì˜ˆì¸¡ - ìµœì¢… ê²°ê³¼\n")
            f.write("=" * 60 + "\n")
            f.write(f"ì™„ë£Œ ì¼ì‹œ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼:\n")
            f.write("-" * 40 + "\n")
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
            if len(results_df) > 0:
                best_model = results_df.iloc[0]
                f.write(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model_name']}\n")
                f.write(f"   ë¦¬ìƒ˜í”Œë§ ë°©ë²•: {best_model['resampling_method'].upper()}\n")
                f.write(f"   Test ROC-AUC: {best_model['roc_auc']:.4f}\n")
                f.write(f"   Test F1-Score: {best_model['f1_score']:.4f}\n")
                f.write(f"   Test Precision: {best_model['precision']:.4f}\n")
                f.write(f"   Test Recall: {best_model['recall']:.4f}\n")
                f.write(f"   Test Accuracy: {best_model['accuracy']:.4f}\n\n")
            
            # ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥
            f.write("ğŸ“Š ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ (Test Set):\n")
            f.write("-" * 40 + "\n")
            for _, row in results_df.iterrows():
                f.write(f"{row['model_name']}:\n")
                f.write(f"   ROC-AUC: {row['roc_auc']:.4f}\n")
                f.write(f"   F1-Score: {row['f1_score']:.4f}\n")
                f.write(f"   Precision: {row['precision']:.4f}\n")
                f.write(f"   Recall: {row['recall']:.4f}\n\n")
            
            f.write("ğŸ’¡ ê²°ë¡ :\n")
            f.write("-" * 40 + "\n")
            f.write("- MIMIC-IV ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ 48ì‹œê°„ ì‚¬ë§ë¥  ì˜ˆì¸¡ ëª¨ë¸ë§ ì™„ë£Œ\n")
            f.write("- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•œ ìµœì í™” ìˆ˜í–‰\n")
            f.write("- ëª¨ë“  ëª¨ë¸ê³¼ ê²°ê³¼ê°€ ì¬í˜„ ê°€ëŠ¥í•˜ë„ë¡ ì €ì¥ë¨\n")
            f.write(f"- ìµœì¢… ëª¨ë¸: {self.output_path}\n")
        
        print(f"âœ… ìµœì¢… ë¦¬í¬íŠ¸: {summary_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
    print("=" * 60)
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
    project_root = Path(__file__).parent.parent
    
    # ê²½ë¡œ ì„¤ì • - ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
    modeling_results_path = project_root / "dataset" / "4_modeling"
    resampling_path = project_root / "dataset" / "3_resampled"
    output_path = project_root / "dataset" / "5_final_models"
    
    # íŠœë„ˆ ì´ˆê¸°í™”
    tuner = HyperparameterTuner(modeling_results_path, resampling_path, output_path)
    
    # íŠœë‹ ì‹¤í–‰
    tuner.run_tuning()
    
    # ê²°ê³¼ ì €ì¥
    tuner.save_results()
    
    print("\n" + "=" * 60)
    print("âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {output_path}")
    print("ğŸ‰ MIMIC-IV 48ì‹œê°„ ì‚¬ë§ë¥  ì˜ˆì¸¡ í”„ë¡œì íŠ¸ ì™„ë£Œ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
